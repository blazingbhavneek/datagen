Ready to rock and roll?
I am going to create a dataset generator, it will be a repo
First there will be parser
1. doc py: This one goes through all of the files in a directory and converts it to markdown using minerU and appends output of each to one file, these are text docs: Output Final long md + Vector DB object (i will host a openai compatible embedding model somehwere so send to it for making embeddings)
2. code py: This one will be given input a folder which would be C/CPP/Py code and we will run a parser on it, tree sitter, and make source tree of the whole code base and convert it to database that could be queried with function name, or variable name and it would give me code snippets that have function defs, or function calls, or variable inits or those whole files which contain that: Output Graph DB like object with queriable enties like functions, function calls, variables names, and it should be able to retrieve whole functin defs or the files based on query, this would be object that could be loaded and queried, Also send each function/file etc and get LLM summary of those objects so we can natural language query it, and those should be related to those graph nodes so we can jump across codebase with ease
3. pair: this will take 2 dirs, one of source code and other of documentaton reelated to the code, so it will output both of those things
Then QA generator, this will be Agent which will take data generated by one of 3 of above and make dataset
First pass through all the dataset (docs and code) we will generate questions The questions shoudl be of two type:
1. Easy questions, the ones that could be answered within the given context, no extra info neede
2. Medium questions, that requires knowledge from other chunks thats not visible in current context so we needs to search around for that
once we have the list of questions, each chunks would have atleast 1 (i.e type 2 queston atleast, and try to make 10 each chunk)
Make it two pass, first one generates and the other ones filters nonsense or unrealistic question
Then answer generation The agent will be Iterative For each question an agent will be invoked, it will call the Vector DB iteratively untill the question is answer satisfibly, each answer would be judged and filtered by LLM, if LLM judge says it makes sense, then it will go on to next final stage:
Making dataset
The question will be question, the final answer will be answer, and then intermediate steps where it though and made queries and thought about whether the answer is enough etc will be converted to reasoning field, we will store these steps and create reasonin field which will then becomre reasoning token for fine tuned LLM
Now make a plan on how to implement this, ask qustions first, dont implement it, just make v v detailed plan for each module with IO and a main tester which tests that part on something
Ask questions first whatever you need to know then make a vv detailed plan for each part

Great project! This is a comprehensive system for creating high-quality reasoning datasets from codebases and documentation. Let me ask some clarifying questions before creating a detailed implementation plan:
Questions
Parser Module - Documents (doc.py)
1. MinerU Integration: Are you using the MinerU library for PDF/document parsing? Should I assume it's already installed, or should the setup include installation instructions?
2. Supported Document Formats: Beyond PDFs, what other document formats should be supported? (Word docs, HTML, plain text, etc.?)
3. Vector DB Choice: Which vector database do you prefer? (ChromaDB, Pinecone, Weaviate, FAISS, Qdrant?) Or should this be configurable?
4. Embedding Model: What's the OpenAI-compatible endpoint URL format? Should batch size and chunking strategy be configurable?
5. Chunking Strategy: For the long markdown file, should we chunk it before embedding? If yes, what chunk size/overlap?
Parser Module - Code (code.py)
1. Tree-sitter Languages: Beyond C/C++/Python, should the system be extensible to other languages?
2. Graph DB Choice: What graph database or structure? (NetworkX in-memory, Neo4j, or custom serializable object?)
3. Code Entities: Should we extract: classes, methods, functions, variables, imports, docstrings, comments? Any priority?
4. LLM for Summaries: Same LLM endpoint as embeddings, or different? What's the expected summary format/length?
5. Relationship Types: What relationships should we track? (calls, defines, imports, inherits, uses, etc.?)
Pair Module (pair.py)
1. Code-Doc Linking: How should we link code to documentation? By filename similarity? Explicit mapping file? Semantic similarity?
2. Output Format: Should this produce a unified queryable object or keep them separate with cross-references?
QA Generator
1. Question Distribution: You mentioned "at least 1 type 2 question per chunk, try for 10 each" - is this 10 total or 10 of each type?
2. Chunk Definition: What constitutes a "chunk"? A function? A file? A documentation section? Configurable size?
3. LLM for Generation: Same endpoint for question generation, filtering, and judging? Any specific prompts/temperature preferences?
4. Quality Filters: What makes a question "nonsense or unrealistic"? Should we have specific heuristics?
Answer Generation Agent
1. Iteration Limits: Max number of vector DB queries per question? Timeout limits?
2. Judging Criteria: What makes an answer "satisfactory"? Specific rubric for the LLM judge?
3. Search Strategy: Should the agent use different search strategies (semantic, keyword, hybrid) or stick to one?
Dataset Output
1. Format: What format for final dataset? JSONL? Parquet? CSV? HuggingFace dataset format?
2. Reasoning Token Format: Should reasoning steps be formatted in a specific way (e.g., XML tags, special tokens)?
3. Metadata: What metadata should be preserved? (source files, timestamps, confidence scores, etc.)
Infrastructure
1. Concurrency: Should we support parallel processing? If yes, how many workers?
2. Caching: Should intermediate results be cached to resume failed runs?
3. Logging: What level of logging/monitoring? Progress bars? Debug output?
4. Testing Data: Do you have sample codebases/docs I should use for test cases, or should I suggest common open-source examples?
Please answer these questions and I'll create a detailed implementation plan with module-by-module breakdown, I/O specifications, and testing strategy!

1. already installed
2. PDF, Docs, HTML, MD, TXT, Excels, PPT
3. ChromaDB
4. It will be locally hosted on another pc wiht vllm or something so it will be ip and host, yes keep it configurable as input parameter
5. Keep it 2k, 200 overlap
6. Yes, for now implement for these 3
7. NetworkX in-memory and pickle objects so i can load for RAG later
8. Classes, methods, functions variables are main entities basically when we read docs we should be able to see its defs and how they are called, in each of these, keep track of variables so we see the function if we wanna query variables name, similary files would be entiees whose children will be functions classes and imports
9. Different endpont, it would be chat model, not embedding model, it would be an embedding model
10. all, i should be able to traverse code however i want
11. Code-Doc Linking, keep above type of outputs seperate LLM will call any of them it wants
12. Seperate
13. try for 10 each, filter nonsense one out later
14. For text docs, text chunks with overlaps, for code, functions, classes and file are chunks
15. Yes same endpoint, same model, locally hosted
16. nonsense or unrealistic means either tooo too deep dive or unrelated to chunk or something totally off the context
17. 10 max, no time limit since server might be busy
18. It should cover the question, in satisfactor way, should not be too off the question, not too stray, not too over explanatory
19. Yes, data will be available in one of the three above ways of the db types will be avaible to agent, make sure it knows from the start, based on avaiable tools, explain it what kind of data it has for making queries
20. jsonl, whichever is most compatible with hf sft trainer
21. Keep them seperate field, i will apply in the way fine tuned models expect them later
22. keep all of it
23. yes, use async agent and llm clients and calls, maintain 10 concurrent agents, each would have one LLM call at a time
24. yes, and for later reasoning text making purposes
25. use tqdm and log files for each agent instance, i wanna see it live
26. leave test cases for now, i will have them seperated from main dataset later
Make 2 docs for detailed implementation plan for Parser Module,
QA Generator (given parse module has its outputs)

Answer:

# Detailed Implementation Plan: QA Generator Module

## Overview
The QA Generator Module takes parsed data from the Parser Module and creates a high-quality reasoning dataset through iterative agent-based question generation, answering, and reasoning trace capture.

---

# Detailed Implementation Plan: Parser Module

## Overview
The Parser Module consists of three components that transform raw data (documents and code) into queryable databases for downstream QA generation.

---

## 1. Document Parser (`doc.py`)

### Purpose
Parse various document formats into markdown, create embeddings, and store in ChromaDB for semantic retrieval.

### Input Specification
```python
class DocParserConfig:
    input_dir: str              # Directory containing documents
    output_md_path: str         # Path for consolidated markdown output
    chroma_db_path: str         # Path for ChromaDB persistence
    embedding_endpoint: str     # e.g., "http://192.168.1.100:8000/v1/embeddings "
    embedding_model: str        # Model name for embeddings
    chunk_size: int = 2000      # Characters per chunk
    chunk_overlap: int = 200    # Character overlap between chunks
    supported_formats: List[str] = ['.pdf', '.docx', '.html', '.md', '.txt', '.xlsx', '.pptx']
```

### Output Specification
```python
class DocParserOutput:
    consolidated_md_path: str       # Path to final markdown file
    chroma_collection_name: str     # ChromaDB collection identifier
    chunk_metadata: List[Dict]      # Metadata for each chunk
    total_chunks: int
    processing_log: str             # Path to processing log
```

### Architecture

#### Components

**1. Document Converter**
```python
class DocumentConverter:
    def __init__(self, config: DocParserConfig):
        """Initialize MinerU and format handlers"""
        
    def convert_to_markdown(self, file_path: str) -> str:
        """
        Convert single file to markdown
        
        Returns:
            - Markdown string with metadata header
        """
        
    def _convert_pdf(self, path: str) -> str:
        """Use MinerU for PDF extraction"""
        
    def _convert_docx(self, path: str) -> str:
        """Use python-docx or mammoth"""
        
    def _convert_html(self, path: str) -> str:
        """Use BeautifulSoup + html2text"""
        
    def _convert_xlsx(self, path: str) -> str:
        """Use openpyxl/pandas to convert tables"""
        
    def _convert_pptx(self, path: str) -> str:
        """Use python-pptx to extract text/notes"""
```

**2. Markdown Consolidator**
```python
class MarkdownConsolidator:
    def __init__(self, output_path: str):
        self.output_path = output_path
        self.file_separator = "\n\n" + "="*80 + "\n\n"
        
    def append_document(self, md_content: str, source_file: str):
        """
        Append markdown with metadata header
        
        Header format:
        # SOURCE: {source_file}
        # TIMESTAMP: {iso_timestamp}
        # FORMAT: {original_format}
        ---
        {content}
        """
        
    def finalize(self) -> str:
        """Return path to consolidated file"""
```

**3. Text Chunker**
```python
class SemanticChunker:
    def __init__(self, chunk_size: int, overlap: int):
        """Initialize with size constraints"""
        
    def chunk_markdown(self, md_content: str, source_file: str) -> List[Chunk]:
        """
        Chunk markdown preserving structure
        
        Strategy:
        1. Split on markdown headers first (##, ###, etc.)
        2. If section > chunk_size, split on paragraphs
        3. If paragraph > chunk_size, split on sentences
        4. Maintain overlap with previous chunk
        
        Returns:
            List of Chunk objects with metadata
        """
        
class Chunk:
    id: str                    # UUID
    content: str               # Chunk text
    source_file: str          # Original file path
    chunk_index: int          # Position in source
    start_char: int           # Start position in consolidated md
    end_char: int             # End position
    headers: List[str]        # Markdown headers in hierarchy
    metadata: Dict            # Additional context
```

**4. Embedding Generator**
```python
class EmbeddingGenerator:
    def __init__(self, endpoint: str, model: str):
        """Initialize OpenAI-compatible client"""
        self.client = OpenAI(base_url=endpoint, api_key="dummy")
        self.model = model
        
    async def generate_embeddings(self, chunks: List[Chunk], 
                                  batch_size: int = 32) -> List[np.ndarray]:
        """
        Generate embeddings with batching
        
        Process:
        1. Batch chunks
        2. Send async requests to endpoint
        3. Handle retries with exponential backoff
        4. Return embeddings in same order
        """
        
    async def _embed_batch(self, texts: List[str]) -> List[np.ndarray]:
        """Single batch embedding with error handling"""
```

**5. ChromaDB Manager**
```python
class ChromaDBManager:
    def __init__(self, db_path: str, collection_name: str):
        """Initialize persistent ChromaDB client"""
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )
        
    def add_chunks(self, chunks: List[Chunk], embeddings: List[np.ndarray]):
        """
        Add chunks with embeddings to ChromaDB
        
        Metadata stored per chunk:
        - source_file
        - chunk_index
        - start_char, end_char
        - headers (as JSON string)
        - timestamp
        """
        
    def query(self, query_embedding: np.ndarray, n_results: int = 5) -> List[Dict]:
        """Query vector DB, return chunks with metadata"""
```

#### Main Pipeline

```python
class DocumentParserPipeline:
    def __init__(self, config: DocParserConfig):
        self.config = config
        self.converter = DocumentConverter(config)
        self.consolidator = MarkdownConsolidator(config.output_md_path)
        self.chunker = SemanticChunker(config.chunk_size, config.chunk_overlap)
        self.embedder = EmbeddingGenerator(config.embedding_endpoint, 
                                          config.embedding_model)
        self.chroma = ChromaDBManager(config.chroma_db_path, 
                                     f"docs_{int(time.time())}")
        self.logger = setup_logger("doc_parser")
        
    async def process(self) -> DocParserOutput:
        """
        Main processing pipeline
        
        Steps:
        1. Discover all supported files in input_dir (recursive)
        2. Convert each file to markdown (parallel processing)
        3. Consolidate all markdown files
        4. Chunk the consolidated markdown
        5. Generate embeddings for all chunks (batched)
        6. Store in ChromaDB
        7. Save metadata and return output
        """
        
        # Step 1: File discovery
        files = self._discover_files()
        self.logger.info(f"Found {len(files)} files to process")
        
        # Step 2-3: Convert and consolidate (with progress bar)
        all_chunks = []
        with tqdm(total=len(files), desc="Converting documents") as pbar:
            for file_path in files:
                try:
                    md_content = self.converter.convert_to_markdown(file_path)
                    self.consolidator.append_document(md_content, file_path)
                    
                    # Chunk individual document
                    chunks = self.chunker.chunk_markdown(md_content, file_path)
                    all_chunks.extend(chunks)
                    
                    pbar.update(1)
                except Exception as e:
                    self.logger.error(f"Failed to process {file_path}: {e}")
                    
        consolidated_path = self.consolidator.finalize()
        self.logger.info(f"Created consolidated markdown: {consolidated_path}")
        
        # Step 5: Generate embeddings
        self.logger.info(f"Generating embeddings for {len(all_chunks)} chunks")
        embeddings = await self.embedder.generate_embeddings(all_chunks)
        
        # Step 6: Store in ChromaDB
        self.logger.info("Storing in ChromaDB")
        self.chroma.add_chunks(all_chunks, embeddings)
        
        # Step 7: Return output
        return DocParserOutput(
            consolidated_md_path=consolidated_path,
            chroma_collection_name=self.chroma.collection.name,
            chunk_metadata=[chunk.to_dict() for chunk in all_chunks],
            total_chunks=len(all_chunks),
            processing_log=self.logger.handlers[0].baseFilename
        )
        
    def _discover_files(self) -> List[str]:
        """Recursively find all supported files"""
```

#### Error Handling

```python
class ParsingError(Exception):
    """Base exception for parsing errors"""
    
class EmbeddingError(Exception):
    """Embedding generation failed"""
    
# Implement retry logic with exponential backoff
async def retry_with_backoff(func, max_retries=3, base_delay=1.0):
    """Utility for retrying failed operations"""
```

#### Testing Module

```python
# test_doc_parser.py

async def test_document_converter():
    """Test individual format conversions"""
    # Create sample files for each format
    # Verify markdown output quality
    
async def test_chunking_strategy():
    """Test chunking preserves context"""
    # Test with various markdown structures
    # Verify overlap works correctly
    
async def test_embedding_generation():
    """Test embedding endpoint connection"""
    # Mock endpoint for testing
    # Verify batch processing
    
async def test_chromadb_storage():
    """Test vector DB operations"""
    # Create test collection
    # Add chunks and query
    # Verify retrieval accuracy
    
async def test_full_pipeline():
    """End-to-end test"""
    # Small test directory with mixed formats
    # Run full pipeline
    # Verify all outputs
    # Query ChromaDB for sample content
```

---

## 2. Code Parser (`code.py`)

### Purpose
Parse source code into a queryable graph structure with semantic summaries, enabling traversal by functions, classes, variables, and relationships.

### Input Specification
```python
class CodeParserConfig:
    input_dir: str                  # Root directory of codebase
    output_graph_path: str          # Path to save pickled NetworkX graph
    llm_endpoint: str               # e.g., "http://192.168.1.100:8000/v1/chat/completions "
    llm_model: str                  # Model name for summarization
    embedding_endpoint: str         # For semantic search
    embedding_model: str
    supported_languages: List[str] = ['python', 'cpp', 'c']
    max_workers: int = 10           # Concurrent processing
```

### Output Specification
```python
class CodeParserOutput:
    graph_path: str                 # Path to pickled NetworkX graph
    entity_summary_path: str        # JSON file with all summaries
    statistics: Dict                # Count of entities, relationships
    processing_log: str
    
class CodeGraph:
    """Serializable graph object"""
    graph: nx.DiGraph               # NetworkX directed graph
    
    # Node types: FILE, CLASS, FUNCTION, VARIABLE
    # Each node has attributes:
    # - type: NodeType
    # - name: str
    # - code: str (full text)
    # - start_line: int
    # - end_line: int
    # - file_path: str
    # - summary: str (LLM-generated)
    # - embedding: np.ndarray
    # - language: str
    # - metadata: Dict
    
    # Edge types: CONTAINS, CALLS, DEFINES, USES, IMPORTS, INHERITS
    # Each edge has attributes:
    # - type: EdgeType
    # - context: str (where relationship occurs)
    
    def query_by_name(self, name: str, entity_type: str = None) -> List[Node]:
        """Find entities by exact name match"""
        
    def query_by_semantic(self, query: str, top_k: int = 5) -> List[Node]:
        """Semantic search using embeddings"""
        
    def get_function_definition(self, name: str) -> Optional[Node]:
        """Get function node with full definition"""
        
    def get_function_calls(self, name: str) -> List[Node]:
        """Get all places where function is called"""
        
    def get_variable_usage(self, name: str) -> List[Node]:
        """Get all variable usages and definitions"""
        
    def get_file_entities(self, file_path: str) -> List[Node]:
        """Get all entities in a file"""
        
    def traverse(self, start_node: Node, relationship: str, 
                 max_depth: int = 3) -> List[Node]:
        """Traverse graph from node following relationship type"""
```

### Architecture

#### Components

**1. Tree-sitter Parser Manager**
```python
class TreeSitterManager:
    def __init__(self, languages: List[str]):
        """Initialize tree-sitter parsers for each language"""
        self.parsers = {
            'python': self._init_python_parser(),
            'cpp': self._init_cpp_parser(),
            'c': self._init_c_parser()
        }
        self.queries = {
            'python': self._load_python_queries(),
            'cpp': self._load_cpp_queries(),
            'c': self._load_c_queries()
        }
        
    def parse_file(self, file_path: str, language: str) -> ts.Tree:
        """Parse source file into AST"""
        
    def _init_python_parser(self) -> ts.Parser:
        """Initialize Python tree-sitter parser"""
        
    def _load_python_queries(self) -> Dict[str, ts.Query]:
        """
        Load tree-sitter queries for Python entities
        
        Queries for:
        - function_definition
        - class_definition
        - function_call
        - variable_assignment
        - import_statement
        - variable_reference
        """
```

**2. Entity Extractor**
```python
class EntityExtractor:
    def __init__(self, ts_manager: TreeSitterManager):
        self.ts_manager = ts_manager
        
    def extract_entities(self, file_path: str, tree: ts.Tree, 
                        source_code: str, language: str) -> List[Entity]:
        """
        Extract all code entities from parsed tree
        
        Returns:
            List of Entity objects (functions, classes, variables)
        """
        
    def _extract_functions(self, tree: ts.Tree, source: str) -> List[FunctionEntity]:
        """
        Extract function definitions
        
        For each function:
        - Name
        - Parameters
        - Return type (if typed)
        - Docstring
        - Full code
        - Start/end line
        - Decorators (Python)
        - Variables used within
        """
        
    def _extract_classes(self, tree: ts.Tree, source: str) -> List[ClassEntity]:
        """
        Extract class definitions
        
        For each class:
        - Name
        - Base classes
        - Methods
        - Attributes
        - Docstring
        - Full code
        - Start/end line
        """
        
    def _extract_variables(self, tree: ts.Tree, source: str) -> List[VariableEntity]:
        """
        Extract variable definitions and assignments
        
        Track:
        - Global variables
        - Class attributes
        - Function parameters
        - Local variables (in scope)
        """
        
    def _extract_imports(self, tree: ts.Tree, source: str) -> List[ImportEntity]:
        """Extract import statements"""

class Entity:
    id: str                    # Unique identifier
    type: EntityType           # FUNCTION, CLASS, VARIABLE, FILE, IMPORT
    name: str
    code: str                  # Full source code
    start_line: int
    end_line: int
    file_path: str
    language: str
    metadata: Dict             # Entity-specific metadata
```

**3. Relationship Extractor**
```python
class RelationshipExtractor:
    def __init__(self, ts_manager: TreeSitterManager):
        self.ts_manager = ts_manager
        
    def extract_relationships(self, entities: List[Entity], tree: ts.Tree,
                            source_code: str) -> List[Relationship]:
        """
        Extract relationships between entities
        
        Relationships:
        - CONTAINS: File contains class, class contains method
        - CALLS: Function calls another function
        - DEFINES: Function defines variable
        - USES: Function uses variable
        - IMPORTS: File imports module
        - INHERITS: Class inherits from another
        """
        
    def _find_function_calls(self, entities: List[Entity], 
                            tree: ts.Tree) -> List[Relationship]:
        """
        Find all function calls in the codebase
        
        Track:
        - Caller function/method
        - Called function
        - Call site (line number, context)
        """
        
    def _find_variable_usage(self, entities: List[Entity], 
                            tree: ts.Tree) -> List[Relationship]:
        """Track where variables are used and defined"""
        
    def _find_inheritance(self, class_entities: List[ClassEntity]) -> List[Relationship]:
        """Track class inheritance relationships"""

class Relationship:
    source_id: str             # Source entity ID
    target_id: str             # Target entity ID
    type: RelationshipType     # CONTAINS, CALLS, DEFINES, USES, etc.
    context: str               # Code snippet showing relationship
    line_number: int           # Where relationship occurs
```

**4. Graph Builder**
```python
class GraphBuilder:
    def __init__(self):
        self.graph = nx.DiGraph()
        
    def build_graph(self, entities: List[Entity], 
                   relationships: List[Relationship]) -> nx.DiGraph:
        """
        Construct NetworkX graph from entities and relationships
        
        Process:
        1. Add all entities as nodes with attributes
        2. Add all relationships as edges with attributes
        3. Create hierarchical structure (file -> class -> method)
        4. Add cross-references (function calls, imports)
        """
        
    def _add_entity_node(self, entity: Entity):
        """Add entity as graph node with all attributes"""
        
    def _add_relationship_edge(self, relationship: Relationship):
        """Add relationship as graph edge"""
        
    def _create_file_hierarchy(self, entities: List[Entity]):
        """
        Create containment hierarchy:
        FILE
        ├── IMPORT
        ├── CLASS
        │   ├── METHOD
        │   └── ATTRIBUTE
        └── FUNCTION
            └── VARIABLE
        """
```

**5. LLM Summarizer**
```python
class LLMSummarizer:
    def __init__(self, llm_endpoint: str, model: str):
        """Initialize OpenAI-compatible chat client"""
        self.client = OpenAI(base_url=llm_endpoint, api_key="dummy")
        self.model = model
        
    async def summarize_entities(self, entities: List[Entity], 
                                 batch_size: int = 10) -> Dict[str, str]:
        """
        Generate natural language summaries for entities
        
        Process:
        1. Batch entities by type
        2. Generate prompts based on entity type
        3. Send async requests
        4. Parse and store summaries
        
        Returns:
            Dict mapping entity_id to summary
        """
        
    async def _summarize_function(self, func: FunctionEntity) -> str:
        """
        Summarize function
        
        Prompt template:
        "Analyze this {language} function and provide a concise summary:
        - What does it do?
        - What are the inputs and outputs?
        - Any notable logic or patterns?
        
        Function:
        {code}
        
        Provide a 2-3 sentence summary."
        """
        
    async def _summarize_class(self, cls: ClassEntity) -> str:
        """
        Summarize class
        
        Include:
        - Purpose of the class
        - Key methods
        - Relationships to other classes
        """
        
    async def _summarize_file(self, file_entities: List[Entity]) -> str:
        """
        Summarize entire file based on its contents
        
        Include:
        - Main purpose
        - Key components
        - External dependencies
        """
```

**6. Semantic Indexer**
```python
class SemanticIndexer:
    def __init__(self, embedding_endpoint: str, model: str):
        """Initialize embedding client"""
        self.embedder = EmbeddingGenerator(embedding_endpoint, model)
        
    async def create_embeddings(self, entities: List[Entity], 
                               summaries: Dict[str, str]) -> Dict[str, np.ndarray]:
        """
        Create embeddings for semantic search
        
        For each entity:
        1. Combine: name + summary + first few lines of code
        2. Generate embedding
        3. Store in entity metadata
        
        Returns:
            Dict mapping entity_id to embedding
        """
        
    def _create_embedding_text(self, entity: Entity, summary: str) -> str:
        """
        Create text for embedding
        
        Format:
        "{entity_type}: {name}
        Summary: {summary}
        Code snippet: {first_10_lines}"
        """
```

#### Main Pipeline

```python
class CodeParserPipeline:
    def __init__(self, config: CodeParserConfig):
        self.config = config
        self.ts_manager = TreeSitterManager(config.supported_languages)
        self.entity_extractor = EntityExtractor(self.ts_manager)
        self.relationship_extractor = RelationshipExtractor(self.ts_manager)
        self.graph_builder = GraphBuilder()
        self.summarizer = LLMSummarizer(config.llm_endpoint, config.llm_model)
        self.indexer = SemanticIndexer(config.embedding_endpoint, 
                                      config.embedding_model)
        self.logger = setup_logger("code_parser")
        
    async def process(self) -> CodeParserOutput:
        """
        Main processing pipeline
        
        Steps:
        1. Discover all source files
        2. Parse each file with tree-sitter (parallel)
        3. Extract entities from each file
        4. Extract relationships across files
        5. Build NetworkX graph
        6. Generate LLM summaries for entities
        7. Create embeddings for semantic search
        8. Pickle graph and save metadata
        """
        
        # Step 1: File discovery
        source_files = self._discover_source_files()
        self.logger.info(f"Found {len(source_files)} source files")
        
        # Step 2-3: Parse and extract entities (parallel)
        all_entities = []
        all_trees = {}
        
        with tqdm(total=len(source_files), desc="Parsing source files") as pbar:
            tasks = []
            for file_path, language in source_files:
                task = self._parse_and_extract(file_path, language)
                tasks.append(task)
                
            # Process in batches to limit concurrency
            for batch in batch_tasks(tasks, self.config.max_workers):
                results = await asyncio.gather(*batch, return_exceptions=True)
                for result in results:
                    if isinstance(result, Exception):
                        self.logger.error(f"Parsing failed: {result}")
                    else:
                        entities, tree, file_path = result
                        all_entities.extend(entities)
                        all_trees[file_path] = tree
                    pbar.update(1)
        
        self.logger.info(f"Extracted {len(all_entities)} entities")
        
        # Step 4: Extract relationships
        self.logger.info("Extracting relationships")
        all_relationships = []
        for file_path, tree in all_trees.items():
            source_code = self._read_file(file_path)
            file_entities = [e for e in all_entities if e.file_path == file_path]
            relationships = self.relationship_extractor.extract_relationships(
                file_entities, tree, source_code
            )
            all_relationships.extend(relationships)
            
        self.logger.info(f"Extracted {len(all_relationships)} relationships")
        
        # Step 5: Build graph
        self.logger.info("Building code graph")
        graph = self.graph_builder.build_graph(all_entities, all_relationships)
        
        # Step 6: Generate summaries
        self.logger.info("Generating LLM summaries")
        summaries = await self.summarizer.summarize_entities(all_entities)
        
        # Add summaries to graph nodes
        for entity_id, summary in summaries.items():
            if graph.has_node(entity_id):
                graph.nodes[entity_id]['summary'] = summary
        
        # Step 7: Create embeddings
        self.logger.info("Creating semantic embeddings")
        embeddings = await self.indexer.create_embeddings(all_entities, summaries)
        
        # Add embeddings to graph nodes
        for entity_id, embedding in embeddings.items():
            if graph.has_node(entity_id):
                graph.nodes[entity_id]['embedding'] = embedding
        
        # Step 8: Save outputs
        self.logger.info("Saving code graph")
        code_graph = CodeGraph(graph)
        
        with open(self.config.output_graph_path, 'wb') as f:
            pickle.dump(code_graph, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        # Save summaries separately for inspection
        summary_path = self.config.output_graph_path.replace('.pkl', '_summaries.json')
        with open(summary_path, 'w') as f:
            json.dump(summaries, f, indent=2)
        
        # Generate statistics
        stats = {
            'total_files': len(source_files),
            'total_entities': len(all_entities),
            'total_relationships': len(all_relationships),
            'entity_breakdown': self._count_entity_types(all_entities),
            'relationship_breakdown': self._count_relationship_types(all_relationships)
        }
        
        return CodeParserOutput(
            graph_path=self.config.output_graph_path,
            entity_summary_path=summary_path,
            statistics=stats,
            processing_log=self.logger.handlers[0].baseFilename
        )
        
    async def _parse_and_extract(self, file_path: str, 
                                language: str) -> Tuple[List[Entity], ts.Tree, str]:
        """Parse single file and extract entities"""
        source_code = self._read_file(file_path)
        tree = self.ts_manager.parse_file(file_path, language)
        entities = self.entity_extractor.extract_entities(
            file_path, tree, source_code, language
        )
        return entities, tree, file_path
        
    def _discover_source_files(self) -> List[Tuple[str, str]]:
        """
        Recursively find all source files
        
        Returns:
            List of (file_path, language) tuples
        """
        
    def _count_entity_types(self, entities: List[Entity]) -> Dict[str, int]:
        """Count entities by type"""
        
    def _count_relationship_types(self, relationships: List[Relationship]) -> Dict[str, int]:
        """Count relationships by type"""
```

#### Query Interface

```python
class CodeGraphQuery:
    """Helper class for querying loaded code graphs"""
    
    def __init__(self, graph_path: str, embedding_endpoint: str, 
                 embedding_model: str):
        with open(graph_path, 'rb') as f:
            self.code_graph: CodeGraph = pickle.load(f)
        self.embedder = EmbeddingGenerator(embedding_endpoint, embedding_model)
        
    async def semantic_search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        Semantic search across code entities
        
        Process:
        1. Generate embedding for query
        2. Compute cosine similarity with all entity embeddings
        3. Return top-k matches with context
        """
        
    def find_function_by_name(self, name: str) -> List[Dict]:
        """Find functions by exact name match"""
        
    def get_function_call_chain(self, function_name: str, max_depth: int = 3) -> Dict:
        """
        Get call chain starting from function
        
        Returns:
            Tree showing what this function calls and is called by
        """
        
    def get_variable_flow(self, var_name: str) -> List[Dict]:
        """Track variable from definition through all uses"""
        
    def get_class_hierarchy(self, class_name: str) -> Dict:
        """Get inheritance hierarchy for a class"""
```

#### Testing Module

```python
# test_code_parser.py

async def test_tree_sitter_parsing():
    """Test tree-sitter parsing for each language"""
    # Sample Python, C, C++ files
    # Verify AST structure
    
async def test_entity_extraction():
    """Test entity extraction accuracy"""
    # Known codebase with documented entities
    # Verify all functions, classes, variables found
    
async def test_relationship_extraction():
    """Test relationship detection"""
    # Code with known call chains
    # Verify CALLS, USES, DEFINES relationships
    
async def test_graph_building():
    """Test graph construction"""
    # Build graph from test entities
    # Verify structure and traversal
    
async def test_llm_summarization():
    """Test summary generation"""
    # Mock LLM endpoint
    # Verify summary quality
    
async def test_semantic_indexing():
    """Test embedding generation"""
    # Create embeddings for test entities
    # Verify semantic similarity
    
async def test_graph_queries():
    """Test query interface"""
    # Load test graph
    # Test various query types
    # Verify result accuracy
    
async def test_full_pipeline():
    """End-to-end test"""
    # Small test codebase
    # Run full pipeline
    # Verify graph can be loaded and queried
```

---

## 3. Pair Parser (`pair.py`)

### Purpose
Combine document and code parsing for projects with both documentation and source code, maintaining separate queryable objects.

### Input Specification
```python
class PairParserConfig:
    code_dir: str
    docs_dir: str
    output_dir: str                 # Directory for all outputs
    doc_config: DocParserConfig     # Reuse doc parser config
    code_config: CodeParserConfig   # Reuse code parser config
```

### Output Specification
```python
class PairParserOutput:
    doc_output: DocParserOutput
    code_output: CodeParserOutput
    manifest_path: str              # JSON manifest linking outputs
```

### Architecture

```python
class PairParserPipeline:
    def __init__(self, config: PairParserConfig):
        self.config = config
        self.doc_parser = DocumentParserPipeline(config.doc_config)
        self.code_parser = CodeParserPipeline(config.code_config)
        self.logger = setup_logger("pair_parser")
        
    async def process(self) -> PairParserOutput:
        """
        Run both parsers in parallel
        
        Steps:
        1. Start doc parser on docs_dir
        2. Start code parser on code_dir (parallel)
        3. Wait for both to complete
        4. Create manifest linking outputs
        5. Return combined output
        """
        
        self.logger.info("Starting paired parsing")
        
        # Run parsers in parallel
        doc_task = self.
doc_parser.process()
        code_task = self.code_parser.process()
        
        doc_output, code_output = await asyncio.gather(doc_task, code_task)
        
        # Create manifest
        manifest = {
            'created_at': datetime.now().isoformat(),
            'code_dir': self.config.code_dir,
            'docs_dir': self.config.docs_dir,
            'doc_output': {
                'markdown': doc_output.consolidated_md_path,
                'chroma_db': self.config.doc_config.chroma_db_path,
                'collection': doc_output.chroma_collection_name,
                'chunks': doc_output.total_chunks
            },
            'code_output': {
                'graph': code_output.graph_path,
                'summaries': code_output.entity_summary_path,
                'statistics': code_output.statistics
            }
        }
        
        manifest_path = os.path.join(self.config.output_dir, 'manifest.json')
        with open(manifest_path, 'w') as f:
            json.dump(manifest, f, indent=2)
        
        self.logger.info(f"Paired parsing complete. Manifest: {manifest_path}")
        
        return PairParserOutput(
            doc_output=doc_output,
            code_output=code_output,
            manifest_path=manifest_path
        )
```

#### Testing Module

```python
# test_pair_parser.py

async def test_parallel_processing():
    """Test both parsers run in parallel"""
    # Test project with docs + code
    # Verify both complete successfully
    
async def test_manifest_creation():
    """Test manifest links all outputs correctly"""
    # Run pair parser
    # Verify manifest structure
    # Load each component using manifest
```

---

## Utilities and Common Components

### Logging Setup
```python
def setup_logger(name: str, log_dir: str = "./logs") -> logging.Logger:
    """
    Create logger with file and console handlers
    
    - Separate log file per agent instance
    - Timestamped entries
    - Configurable log level
    """
```

### Progress Tracking
```python
class ProgressTracker:
    """
    Wrapper around tqdm for consistent progress bars
    
    - Support nested progress bars
    - File-based persistence for resuming
    """
```

### Async Utilities
```python
async def batch_tasks(tasks: List[Coroutine], batch_size: int):
    """Process async tasks in batches"""
    
async def retry_async(func: Callable, max_retries: int = 3, 
                     base_delay: float = 1.0):
    """Retry async function with exponential backoff"""
```

### Configuration Management
```python
class ConfigValidator:
    """Validate configuration objects before processing"""
    
    @staticmethod
    def validate_doc_config(config: DocParserConfig):
        """Ensure all paths exist, endpoints are reachable"""
        
    @staticmethod
    def validate_code_config(config: CodeParserConfig):
        """Validate code parser configuration"""
```

---

## Dependencies

```python
# requirements.txt for Parser Module

# Document parsing
minerU>=0.1.0
python-docx>=0.8.11
beautifulsoup4>=4.12.0
html2text>=2020.1.16
openpyxl>=3.1.0
python-pptx>=0.6.21
pandas>=2.0.0

# Vector DB
chromadb>=0.4.0

# Tree-sitter for code parsing
tree-sitter>=0.20.0
tree-sitter-python>=0.20.0
tree-sitter-cpp>=0.20.0
tree-sitter-c>=0.20.0

# Graph library
networkx>=3.0

# LLM/Embedding clients
openai>=1.0.0
aiohttp>=3.9.0

# Utilities
tqdm>=4.65.0
numpy>=1.24.0
pydantic>=2.0.0

# Testing
pytest>=7.4.0
pytest-asyncio>=0.21.0
```

---

## File Structure

```
parser_module/
├── __init__.py
├── doc.py                  # Document parser
├── code.py                 # Code parser
├── pair.py                 # Paired parser
├── utils/
│   ├── __init__.py
│   ├── logging.py          # Logging utilities
│   ├── progress.py         # Progress tracking
│   ├── async_utils.py      # Async helpers
│   └── config.py           # Configuration validation
├── tests/
│   ├── __init__.py
│   ├── test_doc_parser.py
│   ├── test_code_parser.py
│   ├── test_pair_parser.py
│   └── fixtures/           # Test data
│       ├── sample_docs/
│       └── sample_code/
├── requirements.txt
└── README.md
```





## System Architecture Overview

### Input Specification
```python
class QAGeneratorConfig:
    # Data sources (from Parser Module)
    data_source_type: str           # "doc", "code", or "pair"
    manifest_path: str              # Path to manifest.json (for pair mode)
    
    # For doc mode
    doc_markdown_path: Optional[str]
    chroma_db_path: Optional[str]
    chroma_collection: Optional[str]
    
    # For code mode
    code_graph_path: Optional[str]
    
    # LLM endpoints
    llm_endpoint: str               # Chat completion endpoint
    llm_model: str
    embedding_endpoint: str         # For semantic search
    embedding_model: str
    
    # Question generation parameters
    questions_per_chunk: int = 10   # Target per chunk
    easy_question_ratio: float = 0.3  # 30% easy, 70% medium
    
    # Agent parameters
    max_iterations_per_question: int = 10
    max_concurrent_agents: int = 10
    answer_quality_threshold: float = 0.7  # LLM judge score threshold
    
    # Output
    output_dir: str
    dataset_name: str
    
    # Caching and resumption
    cache_dir: str = "./cache"
    resume_from_checkpoint: bool = False
```

### Output Specification
```python
class QAGeneratorOutput:
    dataset_path: str               # Final JSONL dataset
    statistics_path: str            # Generation statistics
    agent_logs_dir: str             # Individual agent logs
    checkpoint_dir: str             # For resuming
    
class DatasetEntry:
    """Single entry in final dataset"""
    id: str                         # Unique identifier
    question: str                   # Generated question
    answer: str                     # Final answer
    reasoning: str                  # Reasoning trace (converted from steps)
    question_type: str              # "easy" or "medium"
    source_chunks: List[str]        # Source chunk IDs
    metadata: Dict                  # Additional context
    quality_scores: Dict            # Judge scores
    generation_timestamp: str

class ReasoningStep:
    """Intermediate reasoning step during answer generation"""
    step_number: int
    thought: str                    # Agent's reasoning
    action: str                     # Tool used (vector_search, graph_query, etc.)
    action_input: Dict              # Parameters for the action
    observation: str                # Result from tool
    self_evaluation: str            # Agent's assessment of progress
```

---

## Phase 1: Question Generation

### Components

**1. Chunk Manager**
```python
class ChunkManager:
    """Manages data chunks from parsed sources"""
    
    def __init__(self, config: QAGeneratorConfig):
        self.config = config
        self.data_source_type = config.data_source_type
        self.chunks: List[Chunk] = []
        
    def load_chunks(self) -> List[Chunk]:
        """
        Load chunks based on data source type
        
        For doc mode:
        - Load from ChromaDB metadata
        - Each chunk is a document chunk
        
        For code mode:
        - Load from NetworkX graph
        - Each "chunk" is a function, class, or file
        
        For pair mode:
        - Load both and merge
        """
        
    def _load_doc_chunks(self) -> List[Chunk]:
        """
        Load document chunks from ChromaDB
        
        Returns:
            List of document chunks with metadata
        """
        
    def _load_code_chunks(self) -> List[Chunk]:
        """
        Load code entities as chunks
        
        Strategy:
        - Functions are chunks
        - Classes are chunks
        - Files are chunks (for high-level questions)
        
        Returns:
            List of code entity chunks
        """
        
    def _load_pair_chunks(self) -> List[Chunk]:
        """Merge doc and code chunks"""

class Chunk:
    id: str                         # Unique identifier
    content: str                    # Text content
    chunk_type: str                 # "doc", "function", "class", "file"
    source_file: str                # Original file
    metadata: Dict                  # Additional context
    embedding: Optional[np.ndarray] # For semantic search
```

**2. Question Generator Agent**
```python
class QuestionGeneratorAgent:
    """LLM-based agent for generating questions"""
    
    def __init__(self, config: QAGeneratorConfig):
        self.config = config
        self.client = OpenAI(base_url=config.llm_endpoint, api_key="dummy")
        self.model = config.llm_model
        self.logger = setup_logger(f"question_gen")
        
    async def generate_questions(self, chunk: Chunk) -> List[Question]:
        """
        Generate questions for a single chunk
        
        Process:
        1. Analyze chunk content and type
        2. Generate mix of easy and medium questions
        3. Ensure diversity and relevance
        
        Returns:
            List of Question objects (both easy and medium)
        """
        
    async def _generate_easy_questions(self, chunk: Chunk, count: int) -> List[Question]:
        """
        Generate easy questions (answerable within chunk)
        
        Prompt template:
        "You are generating questions for a technical dataset.
        
        Context:
        {chunk.content}
        
        Generate {count} questions that can be answered DIRECTLY from this context.
        These should be straightforward questions where all information needed
        is present in the context above.
        
        Requirements:
        - Questions should be specific and clear
        - Answers should be contained in the context
        - Mix of factual, conceptual, and procedural questions
        - Avoid yes/no questions
        
        Return questions in JSON format:
        [
          {{
            "question": "...",
            "question_type": "easy",
            "rationale": "why this is a good question"
          }}
        ]"
        """
        
    async def _generate_medium_questions(self, chunk: Chunk, count: int) -> List[Question]:
        """
        Generate medium questions (require multi-chunk reasoning)
        
        Prompt template:
        "You are generating questions for a technical dataset.
        
        Context:
        {chunk.content}
        
        Generate {count} questions that REQUIRE additional context beyond what's shown.
        These questions should:
        - Reference concepts/entities in this chunk
        - Require information from related sections or files to answer fully
        - Test understanding of relationships and dependencies
        - Encourage reasoning across multiple sources
        
        Examples of medium questions:
        - "How does function X interact with other components?"
        - "What are the dependencies required for this module to work?"
        - "Compare this approach with the alternative mentioned in related docs"
        
        Return questions in JSON format:
        [
          {{
            "question": "...",
            "question_type": "medium",
            "rationale": "why additional context is needed"
          }}
        ]"
        """
        
    def _parse_llm_response(self, response: str) -> List[Dict]:
        """
        Parse LLM JSON response with error handling
        
        Handle:
        - Markdown code blocks
        - Malformed JSON
        - Missing fields
        """

class Question:
    id: str                         # Unique identifier
    question: str                   # Question text
    question_type: str              # "easy" or "medium"
    source_chunk_id: str            # Originating chunk
    rationale: str                  # Why this is a good question
    generation_timestamp: str
```

**3. Question Filter Agent**
```python
class QuestionFilterAgent:
    """Second-pass filtering of generated questions"""
    
    def __init__(self, config: QAGeneratorConfig):
        self.config = config
        self.client = OpenAI(base_url=config.llm_endpoint, api_key="dummy")
        self.model = config.llm_model
        self.logger = setup_logger(f"question_filter")
        
    async def filter_questions(self, questions: List[Question], 
                              chunk: Chunk) -> List[Question]:
        """
        Filter out low-quality questions
        
        Process:
        1. Batch questions for efficiency
        2. Send to LLM for quality assessment
        3. Filter based on criteria
        
        Returns:
            Filtered list of high-quality questions
        """
        
    async def _assess_question_quality(self, question: Question, 
                                       chunk: Chunk) -> Dict:
        """
        Assess single question quality
        
        Prompt template:
        "Evaluate this question for a technical dataset:
        
        Question: {question.question}
        Type: {question.question_type}
        Source Context: {chunk.content[:500]}...
        
        Assess the question on:
        1. Clarity: Is it clear and unambiguous? (0-1)
        2. Relevance: Is it relevant to the context? (0-1)
        3. Answerable: Can it realistically be answered? (0-1)
        4. Depth: Does it require meaningful reasoning? (0-1)
        5. Specificity: Is it specific enough to have a clear answer? (0-1)
        
        Flag as REJECT if:
        - Too vague or ambiguous
        - Unrelated to context
        - Requires external knowledge not in codebase/docs
        - Yes/no question (unless very insightful)
        - Too trivial or too impossibly complex
        
        Return assessment in JSON:
        {{
          "decision": "ACCEPT" or "REJECT",
          "scores": {{
            "clarity": 0.0-1.0,
            "relevance": 0.0-1.0,
            "answerable": 0.0-1.0,
            "depth": 0.0-1.0,
            "specificity": 0.0-1.0
          }},
          "reasoning": "brief explanation"
        }}"
        """
        
    def _should_keep_question(self, assessment: Dict) -> bool:
        """
        Determine if question should be kept
        
        Keep if:
        - Decision is ACCEPT
        - Average score > 0.6
        - No critical failures (clarity < 0.4, answerable < 0.5)
        """
```

**4. Question Generation Pipeline**
```python
class QuestionGenerationPipeline:
    """Orchestrates question generation process"""
    
    def __init__(self, config: QAGeneratorConfig):
        self.config = config
        self.chunk_manager = ChunkManager(config)
        self.generator = QuestionGeneratorAgent(config)
        self.filter = QuestionFilterAgent(config)
        self.logger = setup_logger("question_generation")
        
    async def generate_all_questions(self) -> List[Question]:
        """
        Generate questions for all chunks
        
        Process:
        1. Load all chunks
        2. For each chunk (parallel):
           a. Generate questions (first pass)
           b. Filter questions (second pass)
        3. Aggregate all questions
        4. Save checkpoint
        
        Returns:
            List of all filtered questions
        """
        
        # Load chunks
        chunks = self.chunk_manager.load_chunks()
        self.logger.info(f"Loaded {len(chunks)} chunks")
        
        # Calculate question targets
        easy_per_chunk = int(self.config.questions_per_chunk * 
                            self.config.easy_question_ratio)
        medium_per_chunk = self.config.questions_per_chunk - easy_per_chunk
        
        self.logger.info(f"Target: {easy_per_chunk} easy + {medium_per_chunk} "
                        f"medium per chunk")
        
        # Generate questions in parallel (with concurrency limit)
        all_questions = []
        
        with tqdm(total=len(chunks), desc="Generating questions") as pbar:
            semaphore = asyncio.Semaphore(self.config.max_concurrent_agents)
            
            async def process_chunk(chunk: Chunk):
                async with semaphore:
                    try:
                        # First pass: generation
                        questions = await self.generator.generate_questions(chunk)
                        
                        # Second pass: filtering
                        filtered = await self.filter.filter_questions(
                            questions, chunk
                        )
                        
                        self.logger.info(
                            f"Chunk {chunk.id}: {len(questions)} generated, "
                            f"{len(filtered)} kept"
                        )
                        
                        pbar.update(1)
                        return filtered
                        
                    except Exception as e:
                        self.logger.error(f"Failed on chunk {chunk.id}: {e}")
                        pbar.update(1)
                        return []
            
            tasks = [process_chunk(chunk) for chunk in chunks]
            results = await asyncio.gather(*tasks)
            
            for result in results:
                all_questions.extend(result)
        
        self.logger.info(f"Total questions generated: {len(all_questions)}")
        
        # Save checkpoint
        checkpoint_path = os.path.join(
            self.config.cache_dir, 
            f"{self.config.dataset_name}_questions.jsonl"
        )
        self._save_questions(all_questions, checkpoint_path)
        
        return all_questions
        
    def _save_questions(self, questions: List[Question], path: str):
        """Save questions to JSONL for checkpointing"""
        with open(path, 'w') as f:
            for q in questions:
                f.write(json.dumps(q.__dict__) + '\n')
```

---

## Phase 2: Answer Generation (Iterative Agent)

### Components

**1. Tool Suite for Agents**
```python
class ToolRegistry:
    """Registry of tools available to answer generation agents"""
    
    def __init__(self, config: QAGeneratorConfig):
        self.config = config
        self.tools = {}
        self._register_tools()
        
    def _register_tools(self):
        """Register all available tools based on data source type"""
        
        if self.config.data_source_type in ["doc", "pair"]:
            self.tools['vector_search_docs'] = VectorSearchTool(
                self.config.chroma_db_path,
                self.config.chroma_collection,
                self.config.embedding_endpoint,
                self.config.embedding_model
            )
            
        if self.config.data_source_type in ["code", "pair"]:
            self.tools['query_code_graph'] = CodeGraphQueryTool(
                self.config.code_graph_path,
                self.config.embedding_endpoint,
                self.config.embedding_model
            )
            self.tools['get_function_definition'] = FunctionDefinitionTool(
                self.config.code_graph_path
            )
            self.tools['get_function_calls'] = FunctionCallsTool(
                self.config.code_graph_path
            )
            self.tools['get_variable_usage'] = VariableUsageTool(
                self.config.code_graph_path
            )
            self.tools['traverse_code_graph'] = GraphTraversalTool(
                self.config.code_graph_path
            )
    
    def get_tool_descriptions(self) -> str:
        """
        Generate tool descriptions for agent prompt
        
        Returns formatted string describing each tool's:
        - Name
        - Description
        - Input parameters
        - Output format
        """
        
    def execute_tool(self, tool_name: str, **kwargs) -> str:
        """Execute a tool and return result"""

class VectorSearchTool:
    """Tool for semantic search in document chunks"""
    
    def __init__(self, chroma_db_path: str, collection: str, 
                 embedding_endpoint: str, embedding_model: str):
        self.client = chromadb.PersistentClient(path=chroma_db_path)
        self.collection = self.client.get_collection(name=collection)
        self.embedder = EmbeddingGenerator(embedding_endpoint, embedding_model)
        
    async def search(self, query: str, n_results: int = 5) -> List[Dict]:
        """
        Semantic search in document chunks
        
        Returns:
            List of matching chunks with content and metadata
        """
        
    def get_description(self) -> Dict:
        """
        Tool description for agent
        
        Returns:
        {{
          "name": "vector_search_docs",
          "description": "Search documentation using semantic similarity. 
                         Use this to find relevant sections of documentation.",
          "parameters": {{
            "query": "search query string",
            "n_results": "number of results to return (default: 5)"
          }},
          "returns": "List of document chunks with content and metadata"
        }}
        """

class CodeGraphQueryTool:
    """Tool for querying code graph"""
    
    def __init__(self, graph_path: str, embedding_endpoint: str, 
                 embedding_model: str):
        with open(graph_path, 'rb') as f:
            self.code_graph = pickle.load(f)
        self.embedder = EmbeddingGenerator(embedding_endpoint, embedding_model)
        
    async def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        Semantic search across code entities
        
        Returns:
            List of code entities (functions, classes) with summaries
        """
        
    def get_description(self) -> Dict:
        """Tool description for agent"""

class FunctionDefinitionTool:
    """Tool for retrieving function definitions"""
    
    def __init__(self, graph_path: str):
        with open(graph_path, 'rb') as f:
            self.code_graph = pickle.load(f)
            
    def get_definition(self, function_name: str) -> Optional[Dict]:
        """
        Get full function definition
        
        Returns:
            {{
              "name": function_name,
              "code": full_source_code,
              "file": file_path,
              "summary": llm_summary,
              "parameters": [...],
              "line_numbers": (start, end)
            }}
        """

class FunctionCallsTool:
    """Tool for finding where functions are called"""
    
    def __init__(self, graph_path: str):
        with open(graph_path, 'rb') as f:
            self.code_graph = pickle.load(f)
            
    def get_calls(self, function_name: str) -> List[Dict]:
        """
        Find all locations where function is called
        
        Returns:
            List of call sites with context
        """

class VariableUsageTool:
    """Tool for tracking variable usage"""
    
    def __init__(self, graph_path: str):
        with open(graph_path, 'rb') as f:
            self.code_graph = pickle.load(f)
            
    def get_usage(self, variable_name: str) -> List[Dict]:
        """Find where variable is defined and used"""

class GraphTraversalTool:
    """Tool for traversing code graph relationships"""
    
    def __init__(self, graph_path: str):
        with open(graph_path, 'rb') as f:
            self.code_graph = pickle.load(f)
            
    def traverse(self, start_entity: str, relationship: str, 
                max_depth: int = 3) -> Dict:
        """
        Traverse graph from starting entity
        
        Parameters:
        - start_entity: function/class name
        - relationship: "calls", "uses", "contains", "inherits"
        - max_depth: traversal depth
        
        Returns:
            Tree structure of related entities
        """
```

**2. Answer Generation Agent**
```python
class AnswerGenerationAgent:
    """
    Iterative agent that uses tools to answer questions
    
    Uses ReAct-style prompting: Thought -> Action -> Observation loop
    """
    
    def __init__(self, agent_id: str, config: QAGeneratorConfig, 
                 tool_registry: ToolRegistry):
        self.agent_id = agent_id
        self.config = config
        self.tools = tool_registry
        self.client = OpenAI(base_url=config.llm_endpoint, api_key="dummy")
        self.model = config.llm_model
        self.logger = setup_logger(f"agent_{agent_id}")
        
    async def answer_question(self, question: Question) -> AnswerResult:
        """
        Answer question through iterative tool use
        
        Process:
        1. Initialize with question and available tools
        2. Enter ReAct loop:
           a. Agent thinks about next step
           b. Agent chooses tool and parameters
           c. Execute tool, observe result
           d. Agent evaluates if answer is complete
        3. Repeat until answer complete or max iterations
        4. Extract final answer and reasoning trace
        
        Returns:
            AnswerResult with answer and reasoning steps
        """
        
        self.logger.info(f"Answering question: {question.id}")
        
        # Initialize conversation history
        conversation = []
        reasoning_steps = []
        
        # System prompt
        system_prompt = self._build_system_prompt()
        
        # Initial user message
        initial_message = self._build_initial_message(question)
        conversation.append({"role": "user", "content": initial_message})
        
        iteration = 0
        answer_complete = False
        
        while iteration < self.config.max_iterations_per_question and not answer_complete:
            iteration += 1
            self.logger.info(f"Question {question.id} - Iteration {iteration}")
            
            # Get agent's response
            response = await self._call_llm(system_prompt, conversation)
            
            # Parse response to extract thought, action, and action input
            parsed = self._parse_agent_response(response)
            
            if parsed['type'] == 'final_answer':
                # Agent has decided it has enough information
                answer_complete = True
                final_answer = parsed['answer']
                self_evaluation = parsed.get('evaluation', '')
                
                reasoning_steps.append(ReasoningStep(
                    step_number=iteration,
                    thought=parsed['thought'],
                    action="finalize_answer",
                    action_input={},
                    observation=final_answer,
                    self_evaluation=self_evaluation
                ))
                
            elif parsed['type'] == 'tool_use':
                # Agent wants to use a tool
                tool_name = parsed['tool_name']
                tool_input = parsed['tool_input']
                thought = parsed['thought']
                
                # Execute tool
                try:
                    observation = await self.tools.execute_tool(
                        tool_name, **tool_input
                    )
                    observation_str = json.dumps(observation, indent=2)
                    
                except Exception as e:
                    observation_str = f"Error executing tool: {str(e)}"
                    self.logger.error(f"Tool execution failed: {e}")
                
                # Agent evaluates the observation
                eval_response = await self._get_self_evaluation(
                    question, thought, observation_str, reasoning_steps
                )
                
                reasoning_steps.append(ReasoningStep(
                    step_number=iteration,
                    thought=thought,
                    action=tool_name,
                    action_input=tool_input,
                    observation=observation_str,
                    self_evaluation=eval_response
                ))
                
                # Add to conversation
                conversation.append({
                    "role": "assistant",
                    "content": f"Thought: {thought}\nAction: {tool_name}\n"
                              f"Action Input: {json.dumps(tool_input)}"
                })
                conversation.append({
                    "role": "user",
                    "content": f"Observation: {observation_str}\n\n"
                              f"Self-evaluation: {eval_response}\n\n"
                              f"Continue with next thought and action, or provide "
                              f"final answer if sufficient."
                })
                
            else:
                # Malformed response
                self.logger.warning(f"Malformed agent response: {response}")
                conversation.append({
                    "role": "user",
                    "content": "Your response was malformed. Please use the "
                              "specified format: Thought -> Action -> Action Input"
                })
        
        # If max iterations reached without final answer, extract best attempt
        if not answer_complete:
            self.logger.warning(f"Max iterations reached for question {question.id}")
            final_answer = self._extract_best_answer_from_steps(reasoning_steps)
        
        return AnswerResult(
            question_id=question.id,
            answer=final_answer,
            reasoning_steps=reasoning_steps,
            iterations=iteration,
            completed=answer_complete
        )
        
    def _build_system_prompt(self) -> str:
        """
        Build system prompt for answer generation agent
        
        Includes:
        - Role description
        - Available tools
        - Output format requirements
        - ReAct framework explanation
        """
        
        tool_descriptions = self.tools.get_tool_descriptions()
        
        return f"""You are a technical expert assistant tasked with answering questions 
about a codebase and/or documentation. You have access to tools to search and retrieve 
information.

Your data sources:
{self._describe_data_sources()}

Available tools:
{tool_descriptions}

Process:
1. Think step by step about what information you need
2. Use tools iteratively to gather information
3. After each tool use, evaluate if you have enough information
4. Once satisfied, provide a comprehensive final answer

Output format for tool use:
Thought: [your reasoning about what to do next]
Action: [tool name]
Action Input: {{"param1": "value1", "param2": "value2"}}

Output format for final answer:
Thought: [reasoning why you have enough information]
Final Answer: [comprehensive answer to the question]

Important:
- Be thorough but efficient with tool calls
- Synthesize information from multiple sources when needed
- Admit uncertainty if information is insufficient
- Provide specific code references when available
"""
        
    def _build_initial_message(self, question: Question) -> str:
        """
        Build initial user message with question context
        """
        
        return f"""Question: {question.question}

Question Type: {question.question_type}
Source Context: {question.source_chunk_id}

Please answer this question using the available tools. Start with your first thought 
and action.
"""
        
    async def _call_llm(self, system_prompt: str, 
                       conversation: List[Dict]) -> str:
        """Call LLM with system prompt and conversation history"""
        
        messages = [{"role": "system", "content": system_prompt}] + conversation
        
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.7,
            max_tokens=2000
        )
        
        return response.choices[0].message.content
        
    def _parse_agent_response(self, response: str) -> Dict:
        """
        Parse agent response to extract structured information
        
        Handles:
        - Tool use format
        - Final answer format
        - Malformed responses
        
        Returns:
            Dict with type and extracted fields
        """
        
        # Check for final answer
        if "Final Answer:" in response:
            parts = response.split("Final Answer:")
            thought_part = parts[0].replace("Thought:", "").strip()
            answer_part = parts[1].strip()
            
            # Extract evaluation if present
            evaluation = ""
            if "Evaluation:" in thought_part:
                thought_parts = thought_part.split("Evaluation:")
                thought_part = thought_parts[0].strip()
                evaluation = thought_parts[1].strip()
            
            return {
                'type': 'final_answer',
                'thought': thought_part,
                'answer': answer_part,
                'evaluation': evaluation
            }
        
        # Check for tool use
        try:
            thought = ""
            action = ""
            action_input = {}
            
            if "Thought:" in response:
                thought = response.split("Thought:")[1].split("Action:")[0].strip()
            
            if "Action:" in response:
                action = response.split("Action:")[1].split("Action Input:")[0].strip()
            
            if "Action Input:" in response:
                action_input_str = response.split("Action Input:")[1].strip()
                # Remove markdown code blocks if present
                action_input_str = action_input_str.replace("```json", "").replace("```", "")
                action_input = json.loads(action_input_str)
            
            return {
                'type': 'tool_use',
                'thought': thought,
                'tool_name': action,
                'tool_input': action_input
            }
            
        except Exception as e:
            self.logger.error(f"Failed to parse response: {e}\nResponse: {response}")
            return {'type': 'malformed'}
    
    async def _get_self_evaluation(self, question: Question, thought: str,
                                   observation: str, 
                                   previous_steps: List[ReasoningStep]) -> str:
        """
        Agent evaluates whether observation brings it closer to answer
        
        Prompt:
        - Shows question
        - Shows current thought and observation
        - Shows previous steps
        - Asks: Do you have enough to answer? What's missing?
        """
        
        eval_prompt = f"""Question: {question.question}

Your current thought: {thought}

Observation from tool: {observation}

Previous steps: {len(previous_steps)}

Evaluate:
1. Does this observation help answer the question?
2. What information do you now have?
3. What is still missing?
4. Can you answer the question now, or do you need more information?

Provide a brief evaluation (2-3 sentences).
"""
        
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": eval_prompt}],
            temperature=0.3,
            max_tokens=200
        )
        
        return response.choices[0].message.content
        
    def _extract_best_answer_from_steps(self, 
                                       steps: List[ReasoningStep]) -> str:
        """
        If max iterations reached, synthesize best answer from steps
        
        Reviews all observations and generates answer
        """
        
        # Combine all observations
        all_info = "\n\n".join([
            f"Step {s.step_number}:\nThought: {s.thought}\n"
            f"Observation: {s.observation}\nEvaluation: {s.self_evaluation}"
            for s in steps
        ])
        
        return f"[Incomplete - Max iterations reached]\n\nGathered information:\n{all_info}"

class AnswerResult:
    question_id: str
    answer: str
    reasoning_steps: List[ReasoningStep]
    iterations: int
    completed: bool                 # True if reached final answer
```

**3. Answer Quality Judge**
```python
class AnswerQualityJudge:
    """LLM judge to evaluate answer quality"""
    
    def __init__(self, config: QAGeneratorConfig):
        self.config = config
        self.client = OpenAI(base_url=config.llm_endpoint, api_key="dummy")
        self.model = config.llm_model
        self.logger = setup_logger("answer_judge")
        
    async def evaluate_answer(self, question: Question, 
                             answer_result: AnswerResult) -> JudgeScore:
        """
        Evaluate answer quality
        
        Criteria:
        1. Completeness: Does it address all parts of the question?
        2. Accuracy: Is the information correct based on sources?
        3. Relevance: Does it stay on topic?
        4. Clarity: Is it well-explained?
        5. Specificity: Does it provide specific details/examples?




**3. Answer Quality Judge** (continued)
```python
        """
        Evaluate answer quality
        
        Criteria:
        1. Completeness: Does it address all parts of the question?
        2. Accuracy: Is the information correct based on sources?
        3. Relevance: Does it stay on topic?
        4. Clarity: Is it well-explained?
        5. Specificity: Does it provide specific details/examples?
        6. Reasoning: Is the reasoning trace logical?
        
        Prompt template:
        "You are an expert judge evaluating answers to technical questions.
        
        Question: {question.question}
        Question Type: {question.question_type}
        
        Answer:
        {answer_result.answer}
        
        Reasoning Steps Taken: {answer_result.iterations} iterations
        
        Reasoning Trace:
        {self._format_reasoning_steps(answer_result.reasoning_steps)}
        
        Evaluate this answer on these criteria:
        1. COMPLETENESS (0-1): Does it fully answer the question?
        2. ACCURACY (0-1): Is the information factually correct?
        3. RELEVANCE (0-1): Does it stay on topic?
        4. CLARITY (0-1): Is it well-structured and clear?
        5. SPECIFICITY (0-1): Does it reference specific code/docs?
        6. REASONING_QUALITY (0-1): Are the reasoning steps logical?
        
        Overall Score: Average of all criteria
        
        Flag as REJECT if:
        - Overall score < 0.6
        - Accuracy < 0.5
        - Relevance < 0.5
        - Answer is too generic/unhelpful
        
        Provide detailed justification for each score.
        
        Return JSON:
        {{
          "decision": "ACCEPT" or "REJECT",
          "overall_score": 0.0-1.0,
          "criteria_scores": {{
            "completeness": 0.0-1.0,
            "accuracy": 0.0-1.0,
            "relevance": 0.0-1.0,
            "clarity": 0.0-1.0,
            "specificity": 0.0-1.0,
            "reasoning_quality": 0.0-1.0
          }},
          "justification": "detailed explanation",
          "suggested_improvements": "if any"
        }}
        """
        
        # Parse LLM response and return structured score
        
    def _format_reasoning_steps(self, steps: List[ReasoningStep]) -> str:
        """Format reasoning steps for judge prompt"""
        formatted = []
        for step in steps:
            formatted.append(
                f"Step {step.step_number}:\n"
                f"Thought: {step.thought}\n"
                f"Action: {step.action}\n"
                f"Self-evaluation: {step.self_evaluation}\n"
                f"Observation: {step.observation[:200]}..."
            )
        return "\n\n".join(formatted)

class JudgeScore:
    decision: str                   # ACCEPT or REJECT
    overall_score: float
    criteria_scores: Dict[str, float]
    justification: str
    suggested_improvements: str
```

**4. Answer Generation Pipeline**
```python
class AnswerGenerationPipeline:
    """Orchestrates answer generation for all questions"""
    
    def __init__(self, config: QAGeneratorConfig, questions: List[Question]):
        self.config = config
        self.questions = questions
        self.tool_registry = ToolRegistry(config)
        self.judge = AnswerQualityJudge(config)
        self.logger = setup_logger("answer_generation")
        self.checkpoint_dir = os.path.join(config.cache_dir, "answer_checkpoints")
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
    async def generate_all_answers(self) -> List[AnswerResult]:
        """
        Generate answers for all questions in parallel
        
        Process:
        1. Create answer agents (limited concurrency)
        2. Assign questions to agents
        3. Generate answers (parallel)
        4. Judge each answer quality
        5. Filter low-quality answers
        6. Save checkpoints periodically
        
        Returns:
            List of accepted AnswerResults
        """
        
        self.logger.info(f"Generating answers for {len(self.questions)} questions")
        
        # Process in batches to manage concurrency
        semaphore = asyncio.Semaphore(self.config.max_concurrent_agents)
        results = []
        accepted_results = []
        
        with tqdm(total=len(self.questions), desc="Generating answers") as pbar:
            async def process_question(question: Question):
                async with semaphore:
                    agent_id = f"agent_{question.id}"
                    
                    # Create agent instance with its own logger
                    agent = AnswerGenerationAgent(
                        agent_id, self.config, self.tool_registry
                    )
                    
                    try:
                        # Generate answer
                        answer_result: AnswerResult = await agent.answer_question(question)
                        
                        # Judge quality
                        judge_score: JudgeScore = await self.judge.evaluate_answer(
                            question, answer_result
                        )
                        
                        answer_result.quality_score = judge_score
                        
                        # Log results
                        if judge_score.decision == "ACCEPT":
                            accepted_results.append(answer_result)
                            self.logger.info(
                                f"Question {question.id}: ACCEPTED (score={judge_score.overall_score:.2f})"
                            )
                        else:
                            self.logger.info(
                                f"Question {question.id}: REJECTED (score={judge_score.overall_score:.2f})"
                                f" - {judge_score.justification}"
                            )
                        
                        results.append(answer_result)
                        
                        # Save individual agent log
                        log_path = os.path.join(
                            self.checkpoint_dir, 
                            f"agent_{question.id}.log"
                        )
                        with open(log_path, 'w') as f:
                            f.write(self._format_agent_log(answer_result, judge_score))
                        
                        pbar.update(1)
                        return answer_result
                        
                    except Exception as e:
                        self.logger.error(f"Failed on question {question.id}: {e}")
                        pbar.update(1)
                        return None
            
            tasks = [process_question(q) for q in self.questions]
            await asyncio.gather(*tasks, return_exceptions=True)
        
        self.logger.info(
            f"Answer generation complete: {len(accepted_results)} accepted, "
            f"{len(results) - len(accepted_results)} rejected"
        )
        
        # Save checkpoint
        checkpoint_path = os.path.join(
            self.checkpoint_dir,
            f"{self.config.dataset_name}_answers.jsonl"
        )
        self._save_answer_results(results, checkpoint_path)
        
        return accepted_results
    
    def _format_agent_log(self, answer_result: AnswerResult, 
                         judge_score: JudgeScore) -> str:
        """Format detailed log for individual agent run"""
        log_lines = [
            f"Question ID: {answer_result.question_id}",
            f"Iterations: {answer_result.iterations}",
            f"Completed: {answer_result.completed}",
            f"Judge Decision: {judge_score.decision}",
            f"Overall Score: {judge_score.overall_score:.2f}",
            "\n" + "="*80 + "\n",
            "REASONING TRACE:",
            "="*80
        ]
        
        for step in answer_result.reasoning_steps:
            log_lines.extend([
                f"\nStep {step.step_number}:",
                f"  Thought: {step.thought}",
                f"  Action: {step.action}",
                f"  Action Input: {json.dumps(step.action_input, indent=2)}",
                f"  Observation: {step.observation}",
                f"  Self-evaluation: {step.self_evaluation}",
                "-"*40
            ])
        
        log_lines.extend([
            "\n" + "="*80,
            "FINAL ANSWER:",
            "="*80,
            answer_result.answer,
            "\n" + "="*80,
            "JUDGE EVALUATION:",
            "="*80,
            f"Decision: {judge_score.decision}",
            f"Overall Score: {judge_score.overall_score:.2f}",
            "\nCriteria Scores:",
            json.dumps(judge_score.criteria_scores, indent=2),
            "\nJustification:",
            judge_score.justification,
            "\nSuggested Improvements:",
            judge_score.suggested_improvements
        ])
        
        return "\n".join(log_lines)
    
    def _save_answer_results(self, results: List[AnswerResult], path: str):
        """Save answer results to JSONL"""
        with open(path, 'w') as f:
            for result in results:
                f.write(json.dumps({
                    'question_id': result.question_id,
                    'answer': result.answer,
                    'iterations': result.iterations,
                    'completed': result.completed,
                    'quality_score': result.quality_score.__dict__ if result.quality_score else None
                }) + '\n')
```

---

## Phase 3: Dataset Creation

### Components

**1. Reasoning Trace Converter**
```python
class ReasoningTraceConverter:
    """Converts reasoning steps into training-ready format"""
    
    def __init__(self, config: QAGeneratorConfig):
        self.config = config
        self.logger = setup_logger("reasoning_converter")
        
    def convert_to_reasoning_field(self, answer_result: AnswerResult, 
                                   question: Question) -> str:
        """
        Convert reasoning steps to single reasoning field string
        
        Process:
        1. Combine all thoughts and observations
        2. Add special tokens for step separation
        3. Format as conversational flow
        4. Include self-evaluations as internal monologue
        
        Returns:
            Formatted reasoning string for fine-tuning
        """
        
        reasoning_parts = []
        
        # Add initial context
        reasoning_parts.append(
            f"<|context|>I need to answer: {question.question}\n"
            f"The question type is {question.question_type}. "
            f"Available tools: {self._list_available_tools()}<|end_context|>"
        )
        
        # Process each step
        for step in answer_result.reasoning_steps:
            step_block = (
                f"<|step|>{step.step_number}<|end_step|>"
                f"<|thought|>{step.thought}<|end_thought|>"
                f"<|action|>{step.action}<|end_action|>"
                f"<|action_input|>{json.dumps(step.action_input)}<|end_action_input|>"
                f"<|observation|>{step.observation}<|end_observation|>"
                f"<|self_evaluation|>{step.self_evaluation}<|end_self_evaluation|>"
            )
            reasoning_parts.append(step_block)
        
        # Add final answer indicator
        reasoning_parts.append(
            f"<|final_thought|>I have gathered sufficient information to answer "
            f"the question.<|end_final_thought|>"
        )
        
        return "\n".join(reasoning_parts)
    
    def _list_available_tools(self) -> str:
        """List available tools based on data source"""
        if self.config.data_source_type == "doc":
            return "vector_search_docs"
        elif self.config.data_source_type == "code":
            return "query_code_graph, get_function_definition, get_function_calls, get_variable_usage, traverse_code_graph"
        else:  # pair
            return "All document and code tools"
```

**2. Dataset Assembler**
```python
class DatasetAssembler:
    """Assembles final dataset from question-answer pairs"""
    
    def __init__(self, config: QAGeneratorConfig):
        self.config = config
        self.converter = ReasoningTraceConverter(config)
        self.logger = setup_logger("dataset_assembler")
        
    def assemble_dataset(self, questions: List[Question], 
                        answers: List[AnswerResult]) -> List[DatasetEntry]:
        """
        Create final dataset entries
        
        Process:
        1. Match questions with answers
        2. Convert reasoning steps to format
        3. Extract metadata
        4. Create DatasetEntry objects
        
        Returns:
            List of dataset entries ready for JSONL
        """
        
        dataset_entries = []
        question_dict = {q.id: q for q in questions}
        
        for answer in answers:
            question = question_dict.get(answer.question_id)
            if not question:
                self.logger.warning(
                    f"No matching question for answer {answer.question_id}"
                )
                continue
            
            # Convert reasoning trace
            reasoning_field = self.converter.convert_to_reasoning_field(
                answer, question
            )
            
            # Extract source chunks from reasoning steps
            source_chunks = self._extract_source_chunks(answer)
            
            # Create dataset entry
            entry = DatasetEntry(
                id=f"{self.config.dataset_name}_{answer.question_id}",
                question=question.question,
                answer=answer.answer,
                reasoning=reasoning_field,
                question_type=question.question_type,
                source_chunks=source_chunks,
                metadata={
                    'original_question_id': question.id,
                    'source_chunk_id': question.source_chunk_id,
                    'generation_timestamp': datetime.now().isoformat(),
                    'agent_iterations': answer.iterations,
                    'completed': answer.completed,
                    'data_source_type': self.config.data_source_type
                },
                quality_scores=answer.quality_score.criteria_scores if answer.quality_score else {},
                generation_timestamp=datetime.now().isoformat()
            )
            
            dataset_entries.append(entry)
        
        self.logger.info(f"Assembled {len(dataset_entries)} dataset entries")
        return dataset_entries
    
    def _extract_source_chunks(self, answer_result: AnswerResult) -> List[str]:
        """
        Extract source chunk IDs from reasoning steps
        
        From each step's observation, extract which chunks were retrieved
        """
        
        source_chunks = []
        
        for step in answer_result.reasoning_steps:
            # Parse observation to find chunk/entity references
            observation = step.observation
            
            # Different patterns for doc vs code
            if self.config.data_source_type in ["doc", "pair"]:
                # Look for chunk IDs in vector search results
                if "chunk_id" in observation:
                    try:
                        obs_data = json.loads(observation)
                        if isinstance(obs_data, list):
                            for item in obs_data:
                                if 'chunk_id' in item and item['chunk_id'] not in source_chunks:
                                    source_chunks.append(item['chunk_id'])
                    except:
                        pass
            
            if self.config.data_source_type in ["code", "pair"]:
                # Look for code entity IDs
                if "entity_id" in observation or "file_path" in observation:
                    try:
                        obs_data = json.loads(observation)
                        if isinstance(obs_data, list):
                            for item in obs_data:
                                if 'entity_id' in item and item['entity_id'] not in source_chunks:
                                    source_chunks.append(item['entity_id'])
                                elif 'file_path' in item and item['file_path'] not in source_chunks:
                                    source_chunks.append(item['file_path'])
                    except:
                        pass
        
        return source_chunks
    
    def save_dataset(self, entries: List[DatasetEntry], 
                    output_path: str = None) -> str:
        """
        Save dataset to JSONL format
        
        Format:
        Each line is a JSON object with fields:
        - question
        - answer
        - reasoning
        - metadata
        
        Compatible with HuggingFace SFT Trainer
        """
        
        if not output_path:
            output_path = os.path.join(
                self.config.output_dir,
                f"{self.config.dataset_name}.jsonl"
            )
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w') as f:
            for entry in entries:
                f.write(json.dumps({
                    'id': entry.id,
                    'question': entry.question,
                    'answer': entry.answer,
                    'reasoning': entry.reasoning,
                    'question_type': entry.question_type,
                    'source_chunks': entry.source_chunks,
                    'metadata': entry.metadata,
                    'quality_scores': entry.quality_scores
                }) + '\n')
        
        self.logger.info(f"Dataset saved to {output_path}")
        return output_path

---

## Full Pipeline Orchestration

### Main Class
```python
class QAGeneratorPipeline:
    """Main orchestrator for the entire QA generation process"""
    
    def __init__(self, config: QAGeneratorConfig):
        self.config = config
        self.logger = setup_logger("qa_generator_main")
        self._validate_config()
        
    def _validate_config(self):
        """Validate all required paths and endpoints"""
        # Check data source files exist
        if self.config.data_source_type == "doc":
            assert os.path.exists(self.config.doc_markdown_path)
            assert os.path.exists(self.config.chroma_db_path)
        elif self.config.data_source_type == "code":
            assert os.path.exists(self.config.code_graph_path)
        elif self.config.data_source_type == "pair":
            assert os.path.exists(self.config.manifest_path)
        
        # Validate data source type specific paths
        if self.config.data_source_type in ["doc", "pair"]:
            assert self.config.doc_markdown_path
            assert self.config.chroma_db_path
            assert self.config.chroma_collection
        
        if self.config.data_source_type in ["code", "pair"]:
            assert self.config.code_graph_path
        
        # Validate endpoints are accessible
        self._validate_endpoint(self.config.llm_endpoint)
        self._validate_endpoint(self.config.embedding_endpoint)
    
    def _validate_endpoint(self, endpoint: str):
        """Test connectivity to endpoint"""
        # Implementation: try to make a minimal request
    
    async def run_full_pipeline(self) -> QAGeneratorOutput:
        """
        Execute all phases of QA generation
        
        Phase 1: Question Generation
        - Load chunks
        - Generate questions (first pass)
        - Filter questions (second pass)
        
        Phase 2: Answer Generation
        - Initialize agents with tools
        - Generate answers iteratively
        - Judge answer quality
        - Filter low-quality answers
        
        Phase 3: Dataset Creation
        - Convert reasoning traces
        - Assemble dataset entries
        - Save final dataset
        
        Returns:
            QAGeneratorOutput with paths to all outputs
        """
        
        self.logger.info("Starting QA Generation Pipeline")
        self.logger.info(f"Data source type: {self.config.data_source_type}")
        self.logger.info(f"Output directory: {self.config.output_dir}")
        
        # Phase 1: Question Generation
        self.logger.info("=== Phase 1: Question Generation ===")
        
        question_gen_pipeline = QuestionGenerationPipeline(self.config)
        all_questions = await question_gen_pipeline.generate_all_questions()
        
        if not all_questions:
            raise ValueError("No questions generated! Check inputs and configuration.")
        
        # Phase 2: Answer Generation
        self.logger.info("=== Phase 2: Answer Generation ===")
        
        answer_gen_pipeline = AnswerGenerationPipeline(self.config, all_questions)
        accepted_answers = await answer_gen_pipeline.generate_all_answers()
        
        if not accepted_answers:
            raise ValueError("No answers passed quality check!")
        
        # Phase 3: Dataset Creation
        self.logger.info("=== Phase 3: Dataset Creation ===")
        
        dataset_assembler = DatasetAssembler(self.config)
        dataset_entries = dataset_assembler.assemble_dataset(
            all_questions, accepted_answers
        )
        
        # Save final dataset
        dataset_path = dataset_assembler.save_dataset(dataset_entries)
        
        # Generate statistics
        stats_path = self._generate_statistics(dataset_entries)
        
        # Create final output object
        output = QAGeneratorOutput(
            dataset_path=dataset_path,
            statistics_path=stats_path,
            agent_logs_dir=os.path.join(self.config.cache_dir, "answer_checkpoints"),
            checkpoint_dir=os.path.join(self.config.cache_dir, "checkpoints")
        )
        
        self.logger.info(f"Pipeline complete! Dataset: {dataset_path}")
        self.logger.info(f"Total entries: {len(dataset_entries)}")
        
        return output
    
    def _generate_statistics(self, entries: List[DatasetEntry]) -> str:
        """
        Generate comprehensive statistics about the dataset
        
        Statistics include:
        - Total entries
        - Question type distribution
        - Average reasoning steps per entry
        - Average answer length
        - Quality score distribution
        - Data source distribution
        """
        
        stats = {
            'total_entries': len(entries),
            'question_types': {},
            'avg_reasoning_steps': 0,
            'avg_answer_length': 0,
            'avg_quality_score': 0,
            'data_sources': {}
        }
        
        # Calculate statistics
        for entry in entries:
            # Question type
            qtype = entry.question_type
            stats['question_types'][qtype] = stats['question_types'].get(qtype, 0) + 1
            
            # Reasoning steps (count <step> tags)
            reasoning_steps = entry.reasoning.count('<|step|>')
            stats['avg_reasoning_steps'] += reasoning_steps
            
            # Answer length
            stats['avg_answer_length'] += len(entry.answer)
            
            # Quality score
            if entry.quality_scores:
                avg_score = sum(entry.quality_scores.values()) / len(entry.quality_scores)
                stats['avg_quality_score'] += avg_score
            
            # Data source
            source = entry.metadata.get('data_source_type', 'unknown')
            stats['data_sources'][source] = stats['data_sources'].get(source, 0) + 1
        
        # Calculate averages
        if entries:
            stats['avg_reasoning_steps'] /= len(entries)
            stats['avg_answer_length'] /= len(entries)
            stats['avg_quality_score'] /= len(entries)
        
        # Save to file
        stats_path = os.path.join(
            self.config.output_dir,
            f"{self.config.dataset_name}_statistics.json"
        )
        
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=2)
        
        self.logger.info(f"Statistics saved to {stats_path}")
        
        # Print summary
        self.logger.info("\n" + "="*60)
        self.logger.info("DATASET STATISTICS")
        self.logger.info("="*60)
        self.logger.info(f"Total entries: {stats['total_entries']}")
        self.logger.info(f"Question types: {stats['question_types']}")
        self.logger.info(f"Avg reasoning steps: {stats['avg_reasoning_steps']:.2f}")
        self.logger.info(f"Avg answer length: {stats['avg_answer_length']:.2f} chars")
        self.logger.info(f"Avg quality score: {stats['avg_quality_score']:.2f}")
        self.logger.info(f"Data sources: {stats['data_sources']}")
        self.logger.info("="*60)
        
        return stats_path

---

## Testing Module

```python
# test_qa_generator.py

import pytest
import tempfile
import asyncio
from unittest.mock import Mock, patch, AsyncMock

class TestQAGenerator:
    """Comprehensive tests for QA Generator Module"""
    
    @pytest.fixture
    def sample_doc_config(self):
        """Sample document parser config for testing"""
        return QAGeneratorConfig(
            data_source_type="doc",
            doc_markdown_path="test/fixtures/consolidated.md",
            chroma_db_path="test/fixtures/chromadb",
            chroma_collection="test_collection",
            code_graph_path=None,
            manifest_path=None,
            llm_endpoint="http://localhost:8000/v1",
            llm_model="test-model",
            embedding_endpoint="http://localhost:8000/v1",
            embedding_model="test-embedding",
            output_dir=tempfile.mkdtemp(),
            dataset_name="test_dataset",
            cache_dir=tempfile.mkdtemp(),
            max_concurrent_agents=2,
            questions_per_chunk=5
        )
    
    @pytest.fixture
    def sample_code_config(self):
        """Sample code parser config for testing"""
        return QAGeneratorConfig(
            data_source_type="code",
            doc_markdown_path=None,
            chroma_db_path=None,
            chroma_collection=None,
            code_graph_path="test/fixtures/code_graph.pkl",
            manifest_path=None,
            llm_endpoint="http://localhost:8000/v1",
            llm_model="test-model",
            embedding_endpoint="http://localhost:8000/v1",
            embedding_model="test-embedding",
            output_dir=tempfile.mkdtemp(),
            dataset_name="test_dataset_code",
            cache_dir=tempfile.mkdtemp(),
            max_concurrent_agents=2,
            questions_per_chunk=5
        )
    
    @pytest.fixture
    def sample_questions(self):
        """Sample questions for testing"""
        return [
            Question(
                id="q1",
                question="What does the `process_data` function do?",
                question_type="easy",
                source_chunk_id="chunk_123",
                rationale="Simple function doc question",
                generation_timestamp="2026-01-19T10:00:00"
            ),
            Question(
                id="q2",
                question="How does the authentication system integrate with the database layer?",
                question_type="medium",
                source_chunk_id="chunk_456",
                rationale="Requires cross-file reasoning",
                generation_timestamp="2026-01-19T10:00:00"
            )
        ]
    
    async def test_chunk_manager_load_doc(self, sample_doc_config):
        """Test loading document chunks"""
        chunk_manager = ChunkManager(sample_doc_config)
        
        # Mock ChromaDB client
        with patch('chromadb.PersistentClient') as mock_client:
            mock_collection = Mock()
            mock_collection.get.return_value = {
                'ids': ['chunk_1', 'chunk_2'],
                'documents': ['Doc content 1', 'Doc content 2'],
                'metadatas': [
                    {'source_file': 'test.md', 'chunk_index': 0},
                    {'source_file': 'test.md', 'chunk_index': 1}
                ]
            }
            mock_client.return_value.get_collection.return_value = mock_collection
            
            chunks = await chunk_manager.load_chunks()
            
            assert len(chunks) == 2
            assert chunks[0].id == 'chunk_1'
            assert chunks[0].content == 'Doc content 1'
    
    async def test_chunk_manager_load_code(self, sample_code_config):
        """Test loading code chunks"""
        chunk_manager = ChunkManager(sample_code_config)
        
        # Mock NetworkX graph
        with patch('builtins.open', create=True) as mock_open:
            mock_graph = Mock()
            mock_graph.nodes = {
                'func_process_data': {
                    'type': 'FUNCTION',
                    'name': 'process_data',
                    'code': 'def process_data(): pass',
                    'file_path': 'src/processor.py',
                    'summary': 'Processes input data',
                    'embedding': [0.1, 0.2, 0.3]
                }
            }
            mock_open.return_value.__enter__.return_value = Mock()
            mock_pickle = Mock()
            mock_pickle.load.return_value = Mock()
            mock_pickle.load.return_value.graph = mock_graph
            
            with patch('pickle.load', mock_pickle.load):
                chunks = await chunk_manager.load_chunks()
                
                assert len(chunks) == 1
                assert chunks[0].id == 'func_process_data'
                assert 'process_data' in chunks[0].content
    
    @pytest.mark.asyncio
    async def test_question_generator(self, sample_doc_config):
        """Test question generation"""
        generator = QuestionGeneratorAgent(sample_doc_config)
        
        # Mock LLM response
        mock_response = {
            'choices': [{
                'message': {
                    'content': json.dumps([
                        {
                            "question": "What is the purpose of this function?",
                            "question_type": "easy",
                            "rationale": "Directly answerable"
                        }
                    ] * 5)
                }
            }]
        }
        
        with patch.object(generator.client.chat.completions, 'create', 
                         AsyncMock(return_value=Mock(**mock_response))):
            
            chunk = Chunk(
                id="test_chunk",
                content="def process_data():\n    '''Process input data'''\n    pass",
                chunk_type="doc",
                source_file="test.py"
            )
            
            questions = await generator.generate_questions(chunk)
            
            assert len(questions) == 5
            assert all(q.question_type == "easy" for q in questions)
    
    @pytest.mark.asyncio
    async def test_question_filter(self, sample_doc_config):
        """Test question filtering"""
        filter_agent = QuestionFilterAgent(sample_doc_config)
        
        # Mock LLM judge response
        mock_response = {
            'choices': [{
                'message': {
                    'content': json.dumps({
                        "decision": "ACCEPT",
                        "scores": {
                            "clarity": 0.9,
                            "relevance": 0.8,
                            "answerable": 0.9,
                            "depth": 0.7,
                            "specificity": 0.8
                        },
                        "reasoning": "Good question"
                    })
                }
            }]
        }
        
        with patch.object(filter_agent.client.chat.completions, 'create',
                         AsyncMock(return_value=Mock(**mock_response))):
            
            chunk = Chunk(id="c1", content="test", chunk_type="doc", source_file="test.md")
            questions = [
                Question(id="q1", question="Test?", question_type="easy", 
                        source_chunk_id="c1", rationale="test", 
                        generation_timestamp="2026-01-19T10:00:00")
            ]
            
            filtered = await filter_agent.filter_questions(questions, chunk)
            
            assert len(filtered) == 1
            assert filtered[0].id == "q1"
    
    @pytest.mark.asyncio
    async def test_answer_agent_iteration(self, sample_code_config):
        """Test single agent iteration"""
        tool_registry = ToolRegistry(sample_code_config)
        
        agent = AnswerGenerationAgent(
            agent_id="test_agent",
            config=sample_code_config,
            tool_registry=tool_registry
        )
        
        # Mock tool response
        mock_tool_response = [
            {
                'name': 'process_data',
                'summary': 'Processes data',
                'file_path': 'src/processor.py'
            }
        ]
        
        # Mock LLM responses (thought -> tool use -> final answer)
        mock_responses = [
            Mock(choices=[Mock(message=Mock(
                content="Thought: I need to find the function definition\n"
                       "Action: get_function_definition\n"
                       'Action Input: {"function_name": "process_data"}'
            ))]),
            Mock(choices=[Mock(message=Mock(
                content="Observation shows the function processes input data.\n"
                       "Thought: I have enough information\n"
                       "Final Answer: The process_data function processes input data."
            ))])
        ]
        
        with patch.object(agent.client.chat.completions, 'create',
                         AsyncMock(side_effect=mock_responses)):
            
            with patch.object(tool_registry, 'execute_tool',
                             AsyncMock(return_value=json.dumps(mock_tool_response))):
                
                question = Question(
                    id="test_q",
                    question="What does process_data do?",
                    question_type="easy",
                    source_chunk_id="chunk_1"
                )
                
                result = await agent.answer_question(question)
                
                assert result.completed is True
                assert "processes input data" in result.answer
                assert len(result.reasoning_steps) == 2
    
    @pytest.mark.asyncio
    async def test_judge_evaluation(self, sample_doc_config):
        """Test answer quality judging"""
        judge = AnswerQualityJudge(sample_doc_config)
        
        mock_response = {
            'choices': [{
                'message': {
                    'content': json.dumps({
                        "decision": "ACCEPT",
                        "overall_score": 0.85,
                        "criteria_scores": {
                            "completeness": 0.9,
                            "accuracy": 0.8,
                            "relevance": 0.9,
                            "clarity": 0.85
                        },
                        "justification": "Good answer",
                        "suggested_improvements": "None"
                    })
                }
            }]
        }
        
        with patch.object(judge.client.chat.completions, 'create',
                         AsyncMock(return_value=Mock(**mock_response))):
            
            question = Question(
                id="q1",
                question="What is X?",
                question_type="easy",
                source_chunk_id="c1"
            )
            
            answer_result = AnswerResult(
                question_id="q1",
                answer="X is Y",
                reasoning_steps=[],
                iterations=3,
                completed=True
            )
            
            score = await judge.evaluate_answer(question, answer_result)
            
            assert score.decision == "ACCEPT"
            assert score.overall_score == 0.85
    
    def test_reasoning_trace_conversion(self, sample_code_config):
        """Test reasoning trace formatting"""
        converter = ReasoningTraceConverter(sample_code_config)
        
        answer_result = AnswerResult(
            question_id="q1",
            answer="Final answer",
            reasoning_steps=[
                ReasoningStep(
                    step_number=1,
                    thought="Need to search",
                    action="vector_search",
                    action_input={"query": "test"},
                    observation="Found results",
                    self_evaluation="Got some info"
                ),
                ReasoningStep(
                    step_number=2,
                    thought="Now I can answer",
                    action="finalize_answer",
                    action_input={},
                    observation="Final answer",
                    self_evaluation="Done"
                )
            ],
            iterations=2,
            completed=True
        )
        
        question = Question(
            id="q1", 
            question="Test question",
            question_type="easy", 
            source_chunk_id="c1"
        )
        
        reasoning_field = converter.convert_to_reasoning_field(
            answer_result, question
        )
        
        # Verify format includes expected tokens
        assert "<|thought|>" in reasoning_field
        assert "<|action|>" in reasoning_field
        assert "<|observation|>" in reasoning_field
        assert "Test question" in reasoning_field
    
    def test_dataset_assembly(self, sample_doc_config, sample_questions):
        """Test dataset assembly"""
        assembler = DatasetAssembler(sample_doc_config)
        
        answer_results = [
            AnswerResult(
                question_id="q1",
                answer="Answer 1",
                reasoning_steps=[],
                iterations=3,
                completed=True,
                quality_score=JudgeScore(
                    decision="ACCEPT",
                    overall_score=0.8,
                    criteria_scores={},
                    justification="Good",
                    suggested_improvements="None"
                )
            )
        ]
        
        entries = assembler.assemble_dataset(sample_questions[:1], answer_results)
        
        assert len(entries) == 1
        assert entries[0].question == "What does the `process_data` function do?"
        assert entries[0].answer == "Answer 1"
        assert "q1" in entries[0].id
    
    @pytest.mark.asyncio
    async def test_full_pipeline_integration(self, sample_doc_config):
        """Test complete pipeline end-to-end"""
        pipeline = QAGeneratorPipeline(sample_doc_config)
        
        # Mock all components
        with patch.object(pipeline, '_validate_endpoint'), \
             patch('qa_generator.question_generation.QuestionGenerationPipeline') as mock_qg, \
             patch('qa_generator.answer_generation.AnswerGenerationPipeline') as mock_ag, \
             patch('qa_generator.dataset_creation.DatasetAssembler') as mock_da:
            
            # Mock question generation
            mock_qg_instance = Mock()
            mock_qg_instance.generate_all_questions = AsyncMock(
                return_value=[Mock(id="q1")]
            )
            mock_qg.return_value = mock_qg_instance
            
            # Mock answer generation
            mock_ag_instance = Mock()
            mock_ag_instance.generate_all_answers = AsyncMock(
                return_value=[Mock(
                    question_id="q1",
                    answer="Test answer",
                    reasoning_steps=[],
                    iterations=2,
                    completed=True
                )]
            )
            mock_ag.return_value = mock_ag_instance
            
            # Mock dataset assembly
            mock_da_instance = Mock()
            mock_da_instance.assemble_dataset = Mock(
                return_value=[Mock(id="dataset_1")]
            )
            mock_da_instance.save_dataset = Mock(
                return_value="/tmp/test_dataset.jsonl"
            )
            mock_da.return_value = mock_da_instance
            
            # Run pipeline
            output = await pipeline.run_full_pipeline()
            
            assert os.path.exists(output.dataset_path)
            assert "statistics.json" in output.statistics_path
    
    def test_statistics_generation(self, sample_doc_config):
        """Test statistics generation"""
        pipeline = QAGeneratorPipeline(sample_doc_config)
        
        # Create mock entries
        entries = [
            DatasetEntry(
                id="e1",
                question="Q1",
                answer="A1",
                reasoning="<|step|>1<|end_step|>",
                question_type="easy",
                source_chunks=["c1"],
                metadata={},
                quality_scores={'completeness': 0.9},
                generation_timestamp="2026-01-19T10:00:00"
            ),
            DatasetEntry(
                id="e2",
                question="Q2",
                answer="A2",
                reasoning="<|step|>1<|end_step|><|step|>2<|end_step|>",
                question_type="medium",
                source_chunks=["c2", "c3"],
                metadata={},
                quality_scores={'completeness': 0.8},
                generation_timestamp="2026-01-19T10:00:00"
            )
        ]
        
        with tempfile.TemporaryDirectory() as tmpdir:
            pipeline.config.output_dir = tmpdir
            
            stats_path = pipeline._generate_statistics(entries)
            
            assert os.path.exists(stats_path)
            
            with open(stats_path) as f:
                stats = json.load(f)
            
            assert stats['total_entries'] == 2
            assert stats['question_types']['easy'] == 1
            assert stats['question_types']['medium'] == 1
            assert stats['avg_reasoning_steps'] == 1.5

---

## Command Line Interface

```python
# qa_generator/cli.py

import click
import asyncio
import yaml
from pathlib import Path

@click.group()
def cli():
    """QA Dataset Generator CLI"""
    pass

@cli.command()
@click.option('--config', required=True, type=click.Path(exists=True),
              help='Path to YAML configuration file')
@click.option('--resume/--no-resume', default=False,
              help='Resume from checkpoint if available')
def generate(config: str, resume: bool):
    """
    Generate QA dataset from parsed code/docs
    
    Configuration file should contain:
    - data_source_type
    - paths to parser outputs
    - LLM endpoints
    - generation parameters
    - output settings
    """
    
    # Load configuration
    with open(config) as f:
        config_data = yaml.safe_load(f)
    
    # Create config object
    qa_config = QAGeneratorConfig(
        data_source_type=config_data['data_source_type'],
        manifest_path=config_data.get('manifest_path'),
        doc_markdown_path=config_data.get('doc_markdown_path'),
        chroma_db_path=config_data.get('chroma_db_path'),
        chroma_collection=config_data.get('chroma_collection'),
        code_graph_path=config_data.get('code_graph_path'),
        llm_endpoint=config_data['llm_endpoint'],
        llm_model=config_data['llm_model'],
        embedding_endpoint=config_data['embedding_endpoint'],
        embedding_model=config_data['embedding_model'],
        questions_per_chunk=config_data.get('questions_per_chunk', 10),
        easy_question_ratio=config_data.get('easy_question_ratio', 0.3),
        max_iterations_per_question=config_data.get('max_iterations', 10),
        max_concurrent_agents=config_data.get('max_concurrent', 10),
        answer_quality_threshold=config_data.get('quality_threshold', 0.7),
        output_dir=config_data['output_dir'],
        dataset_name=config_data['dataset_name'],
        cache_dir=config_data.get('cache_dir', './cache'),
        resume_from_checkpoint=resume
    )
    
    # Run pipeline
    async def run():
        pipeline = QAGeneratorPipeline(qa_config)
        return await pipeline.run_full_pipeline()
    
    output = asyncio.run(run())
    
    click.echo(f"✅ Dataset generated: {output.dataset_path}")
    click.echo(f"📊 Statistics: {output.statistics_path}")
    click.echo(f"📝 Agent logs: {output.agent_logs_dir}")

@cli.command()
@click.option('--data-source', type=click.Choice(['doc', 'code', 'pair']),
              required=True)
@click.option('--testdata-dir', type=click.Path(), required=True,
              help='Directory with test data')
def test(data_source: str, testdata_dir: str):
    """
    Run tests on QA generator components
    
    Tests:
    - Chunk loading
    - Question generation
    - Answer generation
    - Dataset assembly
    """
    
    click.echo("Running QA Generator tests...")
    
    # Set up test configuration
    test_config = QAGeneratorConfig(
        data_source_type=data_source,
        doc_markdown_path=f"{testdata_dir}/docs/consolidated.md",
        chroma_db_path=f"{testdata_dir}/docs/chromadb",
        chroma_collection="test_collection",
        code_graph_path=f"{testdata_dir}/code/graph.pkl",
        llm_endpoint="http://localhost:8000/v1",
        llm_model="test-model",
        embedding_endpoint="http://localhost:8000/v1",
        embedding_model="test-embedding",
        questions_per_chunk=5,
        output_dir=tempfile.mkdtemp(),
        dataset_name="test_run",
        cache_dir=tempfile.mkdtemp(),
        max_concurrent_agents=2
    )
    
    # Import and run tests
    pytest_args = [
        '-v',
        '--tb=short',
        f'--testdata={testdata_dir}',
        'qa_generator/tests/test_qa_generator.py'
    ]
    
    exit_code = pytest.main(pytest_args)
    
    if exit_code == 0:
        click.echo("✅ All tests passed!")
    else:
        click.echo("❌ Tests failed!")
        raise click.Abort()

@cli.command()
@click.option('--dataset-path', type=click.Path(exists=True), required=True)
@click.option('--sample-size', default=5)
def inspect(dataset_path: str, sample_size: int):
    """
    Inspect generated dataset
    
    Shows statistics and samples random entries
    """
    
    import random
    
    # Load dataset
    entries = []
    with open(dataset_path) as f:
        for line in f:
            entries.append(json.loads(line))
    
    click.echo(f"Total entries: {len(entries)}")
    
    # Show statistics
    question_types = {}
    for e in entries:
        qtype = e.get('question_type', 'unknown')
        question_types[qtype] = question_types.get(qtype, 0) + 1
    
    click.echo(f"Question types: {question_types}")
    
    # Show samples
    click.echo(f"\n--- Random {sample_size} samples ---\n")
    
    for entry in random.sample(entries, min(sample_size, len(entries))):
        click.echo(f"ID: {entry['id']}")
        click.echo(f"Type: {entry['question_type']}")
        click.echo(f"Question: {entry['question']}")
        click.echo(f"Answer preview: {entry['answer'][:100]}...")
        click.echo(f"Reasoning steps: {entry['reasoning'].count('<|step|>')}")
        click.echo(f"Quality: {entry.get('quality_scores', {})}")
        click.echo("-" * 60)

def main():
    """Entry point"""
    cli()

if __name__ == '__main__':
    main()
```

---

## Dependencies

```python
# requirements.txt for QA Generator Module

# LLM clients
openai>=1.0.0
aiohttp>=3.9.0

# Vector DB
chromadb>=0.4.0

# Graph library
networkx>=3.0

# Async utilities
asyncio>=3.4.3

# Progress tracking
tqdm>=4.65.0

# Data processing
numpy>=1.24.0
pandas>=2.0.0

# Configuration
pydantic>=2.0.0
pyyaml>=6.0

# CLI
click>=8.0.0

# Testing
pytest>=7.4.0
pytest-asyncio>=0.21.0

# For parsing outputs from Parser Module (optional)
tree-sitter>=0.20.0  # If need to re-parse
```

---

## File Structure

```
qa_generator/
├── __init__.py
├── cli.py                      # Command line interface
├── main.py                     # Main pipeline orchestrator
├── question_generation.py      # Phase 1: Question generation
├── answer_generation.py        # Phase 2: Answer generation
├── dataset_creation.py         # Phase 3: Dataset assembly
├── tools/
│   __init__.py
│   vector_search_tool.py       # Document search tool
│   code_graph_tools.py         # Code graph query tools
│   tool_registry.py            # Tool registry
├── agents/
│   __init__.py
│   question_generator.py       # Question generation agent
│   question_filter.py          # Question filtering agent
│   answer_generator.py         # Answer generation agent
│   answer_judge.py             # Answer quality judge
├── utils/
│   __init__.py
│   logging.py                  # Logging utilities
│   async_utils.py              # Async helpers
│   chunk_manager.py            # Chunk loading logic
├── tests/
│   __init__.py
│   test_qa_generator.py        # Comprehensive tests
│   fixtures/
│       sample_docs/
│       sample_code/
├── config/
│   __init__.py
│   example_config.yaml         # Example configuration
├── README.md                   # Module documentation
└── requirements.txt
```

---

## Configuration File Example

```yaml
# example_config.yaml

# Data source configuration
data_source_type: "pair"  # "doc", "code", or "pair"

# For pair mode
manifest_path: "/path/to/parser_output/manifest.json"

# For doc mode (if not using pair)
doc_markdown_path: "/path/to/docs/consolidated.md"
chroma_db_path: "/path/to/chromadb"
chroma_collection: "docs_collection"

# For code mode (if not using pair)
code_graph_path: "/path/to/code_graph.pkl"

# LLM endpoints
llm_endpoint: "http://192.168.1.100:8000/v1"
llm_model: "meta-llama/Meta-Llama-3-70B-Instruct"

embedding_endpoint: "http://192.168.1.100:8000/v1"
embedding_model: "text-embedding-ada-002"

# Generation parameters
questions_per_chunk: 10
easy_question_ratio: 0.3  # 30% easy, 70% medium

# Agent parameters
max_iterations_per_question: 10
max_concurrent_agents: 10
answer_quality_threshold: 0.7

# Output settings
output_dir: "./output/datasets"
dataset_name: "my_codebase_qa_v1"
cache_dir: "./cache"

# Optional: Resume from checkpoint
resume_from_checkpoint: false
```

---

## Testing Strategy

### Unit Tests
- **Chunk loading**: Test loading from ChromaDB and NetworkX graph
- **Question generation**: Mock LLM responses, verify format
- **Question filtering**: Mock judge, verify filtering logic
- **Answer agent**: Test ReAct loop, tool calls, parsing
- **Quality judge**: Mock LLM judge, verify scoring
- **Reasoning conversion**: Verify token format
- **Dataset assembly**: Test JSONL output format

### Integration Tests
- **End-to-end pipeline**: Small codebase + docs, full pipeline
- **Tool accuracy**: Test that tools return correct data from Parser Module outputs
- **Agent performance**: Verify agents converge on answers within iteration limit

### Test Data Setup
```bash
# Create test fixtures
mkdir -p qa_generator/tests/fixtures/
mkdir -p qa_generator/tests/fixtures/sample_docs
mkdir -p qa_generator/tests/fixtures/sample_code

# Sample documents
echo "# Test Documentation\n\nThis is a test." > tests/fixtures/sample_docs/test.md

# Sample code
cat > tests/fixtures/sample_code/test.py << 'EOF'
def process_data(input_var):
    '''Processes input data and returns result'''
    result = input_var * 2
    return result

class DataProcessor:
    def __init__(self):
        self.data = None
    
    def process(self, data):
        self.data = data
        return process_data(data)
EOF

# Run parser on test data first
python -m parser_module.cli parse --docs tests/fixtures/sample_docs --output tests/fixtures/parser_output
python -m parser_module.cli parse --code tests/fixtures/sample_code --output tests/fixtures/parser_output
```

---

## Usage Workflow

### 1. Parse Data
```bash
# Documents only
python -m parser_module.cli parse \
  --docs /path/to/documentation \
  --output /path/to/parser_output/docs

# Code only
python -m parser_module.cli parse \
  --code /path/to/source/code \
  --output /path/to/parser_output/code

# Both (pair)
python -m parser_module.cli parse \
  --docs /path/to/docs \
  --code /path/to/code \
  --output /path/to/parser_output/pair
```

### 2. Configure QA Generation
Create `config.yaml` based on `example_config.yaml`

### 3. Generate Dataset
```bash
# Dry run (test with small sample)
python -m qa_generator.cli generate --config config.yaml --dry-run

# Full run
python -m qa_generator.cli generate --config config.yaml

# Resume from checkpoint
python -m qa_generator.cli generate --config config.yaml --resume
```

### 4. Inspect Results
```bash
# View statistics and samples
python -m qa_generator.cli inspect --dataset-path output/datasets/my_dataset.jsonl

# Convert to HuggingFace Dataset
python -c "
from datasets import load_dataset
dataset = load_dataset('json', data_files='output/datasets/my_dataset.jsonl')
dataset.push_to_hub('myusername/my-dataset')
"
```

### 5. Fine-tune Model
```python
# Use with HuggingFace SFT Trainer
from trl import SFTTrainer
from datasets import load_dataset

dataset = load_dataset('json', data_files='output/datasets/my_dataset.jsonl')

trainer = SFTTrainer(
    model="meta-llama/Meta-Llama-3-70B-Instruct",
    train_dataset=dataset['train'],
    dataset_text_field="reasoning",  # Use reasoning field for training
    max_seq_length=8192,
    ...
)
```

---

## Performance Optimization

### Concurrency Management
- **AsyncIO throughout**: All LLM calls and tool executions are async
- **Semaphores**: Limit concurrent agents to prevent overload
- **Batching**: Embed queries in batches (size 32), process questions in parallel

### Caching
- **Embeddings**: Cache embeddings for repeated queries
- **Tool results**: Cache frequent tool calls (e.g., function definitions)
- **Agent logs**: Write logs incrementally, not buffered

### Monitoring
- **Tqdm progress bars**: Real-time progress for each phase
- **Live agent logs**: Each agent writes to its own log file
- **Statistics dashboard**: Live stats during generation

### Memory Management
- **Streaming processing**: Process chunks in batches, not all at once
- **Garbage collection**: Explicit cleanup after each phase
- **Pickle optimization**: Use highest protocol for faster loading

---

## Error Handling and Recovery

### Checkpoint System
- **Phase 1**: Save questions to JSONL after generation
- **Phase 2**: Save answer results periodically (every 50 questions)
- **Resume logic**: Skip already-processed questions/answers

### Retry Logic
```python
# For embedding/LLM calls
async def call_with_retry(func, max_retries=3, base_delay=1.0):
    for attempt in range(max_retries):
        try:
            return await func()
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            delay = base_delay * (2 ** attempt)
            await asyncio.sleep(delay)
            continue
```

### Fallback Strategies
- **If question generation fails**: Log error, continue with other chunks
- **If answer agent fails**: Mark as failed, continue with other questions
- **If judge fails**: Use default rejection
- **If tool fails**: Return error message to agent, let it adapt

### Validation
- **Input validation**: Check all paths exist, endpoints are reachable
- **Output validation**: Verify dataset format, check for malformed entries
- **Quality validation**: Re-run judge on final dataset sample

---

## Future Enhancements

### Multi-turn Conversations
- Generate follow-up questions based on answers
- Create conversational datasets with context

### Hard Question Generation
- Use LLM to identify gaps in knowledge
- Generate adversarial questions that require deep reasoning

### Human-in-the-loop
- Review interface for generated Q&A pairs
- Human annotation of reasoning traces

### Advanced Search Strategies
- Hybrid search (semantic + keyword)
- Graph neural networks for code understanding
- Cross-modal retrieval (doc ↔ code)

### Dataset Diversity
- Balance question types across codebase
- Ensure coverage of all important modules
- Stratified sampling by complexity

---

## Summary

This QA Generator Module creates a complete pipeline from parsed data to fine-tuning dataset:

1. **Question Generation**: Creates diverse, high-quality questions (easy + medium)
2. **Answer Generation**: Uses iterative agents with tools to answer questions
3. **Quality Control**: Two-pass filtering ensures question and answer quality
4. **Reasoning Capture**: Formats agent thought processes for fine-tuning
5. **Dataset Creation**: Produces HuggingFace-compatible JSONL format

The module is designed to be:
- **Modular**: Each phase can run independently
- **Resumable**: Checkpoint system prevents lost work
- **Observable**: Detailed logging and progress tracking
- **Scalable**: Async processing with configurable concurrency
- **Customizable**: Extensive configuration options

**Next Steps**: Implement parser modules first, then proceed with QA generator implementation.